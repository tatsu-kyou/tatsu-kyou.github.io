<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度学习——LeNet卷积神经网络初探</title>
    <url>/2024/11/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<h2 id="模型介绍："><a href="#模型介绍：" class="headerlink" title="模型介绍："></a>模型介绍：</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/p1.png"
                      alt="image"
                ></p>
<p><strong>简单介绍：</strong> 从网络结构可以看出LeNet对于现在的大模型来说是一个非常小的神经网络，他一共由7个层顺序连接组成。<font color="#845EC2">分别是卷积层、pooling层、卷积层、pooling层和三个全连接层</font>。用现代的深度学习框架来实现代码如下：  </p>
<h2 id="代码实现和解读："><a href="#代码实现和解读：" class="headerlink" title="代码实现和解读："></a>代码实现和解读：</h2><div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></div>

<p><strong>解读：</strong> 这一部分是有关网络的定义，可以看出网络的基本层实现都调用了torch的库，<code>sigmoid()</code> 函数的作用是：<font color="#845EC2">让网络中各层叠加后不会坍缩，因为引入了非线性函数。</font>我们来输出一下网络的各层的结构。</p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t:&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure></div>

<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">Conv2d output shape: 	: torch.Size([1, 6, 28, 28])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 6, 28, 28])</span><br><span class="line">AvgPool2d output shape: : torch.Size([1, 6, 14, 14])</span><br><span class="line">Conv2d output shape: 	: torch.Size([1, 16, 10, 10])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 16, 10, 10])</span><br><span class="line">AvgPool2d output shape: : torch.Size([1, 16, 5, 5])</span><br><span class="line">Flatten output shape: 	: torch.Size([1, 400])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 120])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 120])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 84])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 84])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 10])</span><br></pre></td></tr></table></figure></div>
<p>接下来我们利用沐神的<code>d2l</code>包中的数据集准备函数来下载<code>MNIST</code>数据集。</p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):  <span class="comment"># * 判断变量的类型</span></span><br><span class="line">        net.<span class="built_in">eval</span>()          </span><br><span class="line">        <span class="comment">#? 将网络设置为评估模式, 在此模式下，net会关闭一些特定的训练技巧以确保网络的行为和训练时一致</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)  <span class="comment"># * .to(device)是为了将数据送至指定的设备上进行计算</span></span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]  <span class="comment"># * 这里返回的是预测精度</span></span><br></pre></td></tr></table></figure></div>
<p>以上的这段代码的关键步骤是<font color="#FF6F91">执行了.to(device)操作，上述方法作用的调用可用的GPU进行加速运算</font>。  </p>
<p>接下来这段代码是对net执行训练的方法定义：  </p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)  <span class="comment"># ? 初始化参数</span></span><br><span class="line">            </span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()  <span class="comment"># ? 交叉熵损失函数</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">3</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">2</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()  <span class="comment"># ? 将网络设置为训练模式</span></span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):  <span class="comment"># ? enumerate会返回索引同时返回对应迭代次数时的元素</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span> <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>]*num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span> <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

<p>这段代码非常的长，我们将其分为几个部分来进行解读：  </p>
<p><strong>首先：</strong>  </p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)  <span class="comment"># ? 初始化参数</span></span><br><span class="line">            </span><br><span class="line">    net.apply(init_weights)</span><br></pre></td></tr></table></figure></div>
<p>这一段摘要做的是网络<font color='#FFC75F'>所有参数的初始化</font>。  </p>
<p><strong>其次：</strong></p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">net.to(device)</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.CrossEntropyLoss()  <span class="comment"># ? 交叉熵损失函数</span></span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">3</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">2</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br></pre></td></tr></table></figure></div>
<p>这一段主要是<font color='#FFC75F'>定义了网络训练和结果可视化的必要变量，并将网络放在GPU上进行运行</font>。</p>
<p><strong>接下来：</strong></p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    net.train()  <span class="comment"># ? 将网络设置为训练模式</span></span><br><span class="line">    <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):  <span class="comment"># ? enumerate会返回索引同时返回对应迭代次数时的元素</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">        timer.stop()</span><br><span class="line">        train_l = metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line">        train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">            animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">    test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br></pre></td></tr></table></figure></div>
<p>这一部分是最重要的训练部分：<font color='#00C9A7'>前向传导、计算损失、对损失进行反向传导并计算梯度、根据梯度来更新参数</font>。对每一个样本都进行上述的基本过程。</p>
<p>剩下的部分就是对训练的中间过程进行适当的输出。</p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure></div>
<p>这段代码描述的是网络训练器使用的过程。根据上述参数定义，得到的训练结果如下图：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/p2.png"
                      alt="image"
                ></p>
<h2 id="模型局部最优化："><a href="#模型局部最优化：" class="headerlink" title="模型局部最优化："></a>模型局部最优化：</h2><p>接下来，我想做的是，利用循环和结果可视化来找到这个模型下的局部最优超参数。</p>
<p>–つづけ</p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>CNN</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>东游记(一)--此行伊始</title>
    <url>/2024/11/13/Journey-to-the-East1/</url>
    <content><![CDATA[<h2 id="写在最前"><a href="#写在最前" class="headerlink" title="写在最前"></a>写在最前</h2><p>折腾了好久，可算是把这个简陋的博客带起来了🤣。又回想起了早两年被编程环境支配的日子了。这次的博客是基于GitHub搭的，也许是感觉这样比博客园更高级？之前也没怎么接触过 <code>git</code>，就当是顺带学一下了。</p>
<p>特意开了一个新的 <code>categories</code> 用来记录 <strong>四非菜鸡如我的修士养成之路</strong> 的。哈哈，搞得好像我已经上岸开始写经验贴了一样，<font color='#FFC75F'>もう大丈夫だから、絶対に修士になるよ！</font></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/Journey-to-the-East1/p3.jpg"
                      alt="image"
                ></p>
<h2 id="为何出发"><a href="#为何出发" class="headerlink" title="为何出发"></a>为何出发</h2><p>该死的互联网留学机构，都什么年代了还在吹是个人都能润日。 </p>
<p>我想最初埋下种子是懵懂的大一，年轻时真是啥也不懂，不过从目前来看这个选择是相当正确的。时运感人，我本科班主任（下称“华子哥”）就是一个日留子。因为华子哥刚从海外回来，按照瑶湖女专的规定新任教师都要带一届本科生，同专业的其他班都配备专职辅导员，我这是什么运气😎。第一次班会华子哥就分享了自己的个人了经历，三年前的我一定做梦也想不到，三年后我将抛开所有国内可行的门路，开启东游之旅。</p>
<p>真正驱使我走上这条不归路的又是什么呢？我想，也许是那个明明很菜却又不想比任何人更烂的我在作祟，所以编织出了‘选择大于努力’的借口；也许是二〇年大家众所周知的原因，让国服考研、考公、考编成了三幻神，就业环境在两三年内断崖式崩坏；也许是当年的网络烂梗‘世界那么大 我想去看看’在搞鬼，没有出逃能力的前二十年积蓄了太多动力；也许是想再一次证明，除去高考，自己还有再去全力以赴做一件事的决心和能力。</p>
<p>我很功利，无法认可最开始就知道是毫无价值的事情，所以我无法接受国服的考研模式，润至少学会了一门语言？（政治、英语苦手你就知道我为啥接受不了了）。如果没有接触到润日，估计就是更保研狗拼刺刀或者是找工作去了。</p>
<p>另一方面，我没什么主见，保研、考研、找工作……我可能会在其中反复很跳，结果会怎么样真不太好说，所以我需要的是做减法，不给自己留下太多选项，这也是我在做选择时的一贯做法，<font color='#00C9A7'>永远搞不清自己想要什么，但是清楚的知道自己讨厌啥</font>。24年9月底保研狗上岸的时候，怎么说呢，心里不算平静吧，我的保研分应该是能够着末班车的，但我连系统都没填，也就没啥可焦虑的了嘿。但谁又能说我没有那么一瞬想着也许我本该可以休息呢？</p>
<p>我想我还是应了那句话 <code>心比天高 命比纸薄</code></p>
<h2 id="当下情况"><a href="#当下情况" class="headerlink" title="当下情况"></a>当下情况</h2><p>花了一年的时间通了两语，真是没一点语言天赋，踮起脚勉强够到出愿门槛。不过“速通”完接下来就是我比较擅长的了。相比日语，英语我是真的大苦手，至少现在微积分的日文教程能看懂个七七八八？绩点方面前面也说了我是可能有机会保研，不至于太烂，兴许得感谢我的前女友，至少前几年是读了点书的。ごめんなさい，年前的时候太不成熟，没分开说不定我现在已经躺了。往事不复提，恭喜你已经上岸，接下来就是我的主场了。</p>
<h2 id="当务之急"><a href="#当务之急" class="headerlink" title="当务之急"></a>当务之急</h2><p>等十一月的toeic结束后，就开始做毕设和套磁了，初定在年前搞定，套磁因为要套六所左右，进度没那么快，可以适当的延后。在这期间，同步复习修考的课程，数学四门左右and专业课四门左右，准备八到十门足以应对我想考的所有学校了。明年应该就是就是真正意义上的脱产备考了。<font color="#FF6F91">24年12月开始，属于我的修考之旅算是正式的开始了。</font></p>
<h2 id="暂此歇笔"><a href="#暂此歇笔" class="headerlink" title="暂此歇笔"></a>暂此歇笔</h2><p>我的古怪想法很多，但是真到输出的时候就变哑巴了。也许是因为我的灵魂还是过于干瘪，还得看更多的动漫玩更多的gal（bushi）。专栏不定期更新，少说多做，虽然我知道根本没人看，没什么期待才会偶遇惊喜嘛！じゃね、おやすみなさい～</p>
]]></content>
      <categories>
        <category>东游记</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>记录</tag>
      </tags>
  </entry>
  <entry>
    <title>git command</title>
    <url>/2024/12/01/git-command/</url>
    <content><![CDATA[<p><strong>一下全部内容均搬运于b站up“GeekHour”，仅个人学习使用</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/git-command/Git.png"
                      alt="git command"
                ></p>
]]></content>
      <categories>
        <category>常用命令</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title>Experience in improving spoken Japanese</title>
    <url>/2024/12/02/Experience-in-improving-spoken-Japanese/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>斥巨资5r买来的经验贴，感觉很实用所以整理好了放在自己的blog上，期待有缘人发现，当然如果有兴趣提升日语口语的欢迎和我talk😉😉😉🤩🤩🤩！</p>
<p>感觉提升一门语言听说的最快方法就是创造语境，这份经验贴也是如此，跟着下面的步骤走在国内也可以有语境，但大大大前提是要<code>坚持</code>下来。</p>
<h2 id="第一阶段：掌握单词、短句发音"><a href="#第一阶段：掌握单词、短句发音" class="headerlink" title="第一阶段：掌握单词、短句发音"></a>第一阶段：掌握单词、短句发音</h2><p>推荐时间:2个月<br>b站明王道跟动漫学标日单词<br>【【单词速记】跟动漫学新标日单词(初级上册合集)-哔哩哔哩】<a class="link"   href="https://b23.tv/tEkVa30" >https://b23.tv/tEkVa30<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>  </p>
<p>复述每句动漫台词，每天花一个半小时左右，直到把每个发音都听清</p>
<p>因为我在得到这份经验贴的时候刚过完N2，单词的发音这一块还有点印象的，所以这一块不会花太长的时间。但还是要过一遍的，毕竟基础不牢地动山摇。</p>
<h2 id="第二阶段：熟练不同场景下的对话"><a href="#第二阶段：熟练不同场景下的对话" class="headerlink" title="第二阶段：熟练不同场景下的对话"></a>第二阶段：熟练不同场景下的对话</h2><p>推荐时间：半年<br>最最剧场APP  </p>
<p>要求：必须选择1分钟左右动漫片段，台词十几句的进行配音(不要选择其他素材，后期用的时候动漫特定场景应用会浮现在脑海里，别的素材做不到) </p>
<p>数量：保质保量完成50个作品(每个作品练习时长至少半小时)  </p>
<p>听力巩固：每天听自己配过的作品，最好一个作品看几百遍</p>
<h2 id="第三阶段：日常对话训练"><a href="#第三阶段：日常对话训练" class="headerlink" title="第三阶段：日常对话训练"></a>第三阶段：日常对话训练</h2><p>推荐时间：不在日本的所有时间<br>Hellotalk APP  </p>
<p>找陌生日本人搭话，先文字聊天(学会使用日语输入法，聊几天就有组织句子的感觉了)，然后找人约打电话，最好对方会一些中文，找固定语伴，每周一小时，对话可达日常水平</p>
<h2 id="总结：无他，但手熟尔"><a href="#总结：无他，但手熟尔" class="headerlink" title="总结：无他，但手熟尔"></a>总结：无他，但手熟尔</h2><p>不必留于形式，看完这篇经验贴就把他忘了吧，你需要做的是花更多的时间去记单词、去组织语言、去交流。</p>
<p>日语学习中常会听到一句话<code>“N1过了就算入门了”</code>，是也是这个理，有没有这个N1都无所谓，大胆开口去说就完了！</p>
]]></content>
      <tags>
        <tag>日语</tag>
        <tag>经验贴</tag>
      </tags>
  </entry>
  <entry>
    <title>conda_create</title>
    <url>/2024/12/08/conda-create/</url>
    <content><![CDATA[<h2 id="conda下载"><a href="#conda下载" class="headerlink" title="conda下载"></a>conda下载</h2><p>conda下载没什么需要注意的，也不用按着啥教程来，直接去官网上下载就行，不过这里处于节约内存的考虑，我下载的是 <code>miniconda</code>。相较于anaconda的满血版，miniconda更为轻量级，因为主环境一般也不方便使用(可能是我不会用？)，而其他不同版本的环境下的各个包又是相互独立的，所以个人感觉miniconda就已经够用了，毕竟刚重装系统，内存还是要合理规划使用的。</p>
<p>刚开始下载的时候不知道咋回事下成了anaconda，可能是因为下载链接过于隐秘？这里附上miniconda的下载链接：<a class="link"   href="https://repo.anaconda.com/miniconda/" >https://repo.anaconda.com/miniconda/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> 。除了配置一下安装的目录位置以及按需将conda加入到PATH环境中，其他的就是一路Enter了。</p>
<h2 id="创建一个新的conda环境"><a href="#创建一个新的conda环境" class="headerlink" title="创建一个新的conda环境"></a>创建一个新的conda环境</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>创建环境一般需要指定python的版本，好像不指定就是默认？秉持着用双数版本不用最新的原则，我打算创建一个3.10.x版本的conda环境。初始化的命令如下；</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda create --name &lt;Env_name&gt; python=&lt;version&gt;</span><br></pre></td></tr></table></figure></div>

<p>一般用梯子创建不会有啥问题，这一步到这里就完成了。</p>
<h3 id="下载必要的包"><a href="#下载必要的包" class="headerlink" title="下载必要的包"></a>下载必要的包</h3><p>初始化之后可以在终端看到我们仍然在base环境下，想要进入新的环境需要<font color="#FF6F91">激活它</font>。激活的命令如下：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">active &lt;Env_name&gt;</span><br></pre></td></tr></table></figure></div>

<p>如果太久没有使用而忘记了Env的名字时，可以直接去miniconda的目录下查找，也可以用如下的命令来<font color="#FF6F91">查询已有的环境</font>：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda env list</span><br></pre></td></tr></table></figure></div>

<p>有激活当然就有<font color="#FF6F91">退出命令</font>，虽然我一般直接关闭终端根本用不上这个命令，但还是学一个吧：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure></div>

<p>接下来就按需下载必要的包就可以了，下载包一般会分为pip下载和conda下载，通常情况两种方法都是可以，但是有时只有pip有用，不过也有一些情况用pip下载的包会和其他的包冲突，欸！<font color="#845EC2">环境配置中的包冲突问题一直都是一个玄学问题</font>。下载包的命令格式如下：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda install &lt;package_name&gt;</span><br><span class="line"></span><br><span class="line">pip install &lt;package_name&gt;</span><br></pre></td></tr></table></figure></div>

<p>下载包的时候，常常会出现网络问题，毕竟人家的服务器在海外，如果遇到这种情况可以考虑使用<font color="#FF6F91">镜像地址</font>下载，只需要在后边加上如下的命令就可以了：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda install &lt;package_name&gt; -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br></pre></td></tr></table></figure></div>

<p>检查包是否下载也是某些情况下必要的操作，这里也附上代码：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda list</span><br><span class="line"></span><br><span class="line">pip list</span><br><span class="line"></span><br><span class="line">conda list &lt;package_name&gt;</span><br><span class="line"></span><br><span class="line">pip list &lt;package_name&gt;</span><br></pre></td></tr></table></figure></div>]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>conda</tag>
        <tag>terminal</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Graduation Project--Papers Reading</title>
    <url>/2024/12/25/Papers-Reading-of-Graduation-Project/</url>
    <content><![CDATA[<h2 id="前言杂谈"><a href="#前言杂谈" class="headerlink" title="前言杂谈"></a>前言杂谈</h2><p>读的论文越多才发现自己对课题理解越浅薄，很多问题点没有搞清楚就开始着手实现细节，慢慢地埋下了许多祸根。不过目前读到的论文也普遍存在一个共同的问题——没有开源，实验就都是不可复制的，对实验精度还是普遍存疑的。本着开源精神，毕设的所有代码都将以开源为前提进行编写，尽量做到可轻松复现，结果真实。</p>
<p>OK，闲话少叙，开始吧！</p>
<h2 id="1-基于规则的垃圾邮件过滤比较研究"><a href="#1-基于规则的垃圾邮件过滤比较研究" class="headerlink" title="1. 基于规则的垃圾邮件过滤比较研究"></a>1. 基于规则的垃圾邮件过滤比较研究</h2><p>作者：汤、孙</p>
<p>文章篇幅极短，简要介绍了Ripper算法、C4.5决策树和AdaBoost算法的基本原理后，给出了这三种基于规则的垃圾邮件过滤方法的实验比较结果。结论是AdaBoost算法的效果最好，但实验也发现它的计算复杂度较高，耗时更长。</p>
<p>由于篇幅极短论文基本上没有对任何细节和知识点做更详细的介绍，我认为对我的课题有帮助的地方有一下几点：  </p>
<ol>
<li>简要介绍了基于规则的垃圾邮件过滤的流程。</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/Papers-Reading-of-Graduation-Project/%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E9%82%AE%E4%BB%B6%E8%BF%87%E6%BB%A4%E6%B5%81%E7%A8%8B.jpg"
                      alt="基于规则的邮件过滤流程"
                ></p>
<ol start="2">
<li>提到了特征选择后应该做特征提取来降维。</li>
<li>AdaBoost算法在众多基于规则的过滤算法中效果最好，但实验也发现它的计算复杂度较高，耗时更长。</li>
</ol>
<h2 id="2-基于多过滤器集成学习的在线垃圾邮件过滤"><a href="#2-基于多过滤器集成学习的在线垃圾邮件过滤" class="headerlink" title="2. 基于多过滤器集成学习的在线垃圾邮件过滤"></a>2. 基于多过滤器集成学习的在线垃圾邮件过滤</h2><p>作者：刘、王<br>时间：2008</p>
<p>文章详略得当，提出了多学习器集成学习的过滤模型，并通过实验证明了集成学习的模型精度优于单一模型。但是继续阅读文献会发现集成学习的模型精度并不总是能优于单个模型中的最佳模型，需要选择合适的学个体学习器进行集成才能达到较为理想的效果。</p>
<p>由于论文并没有做集成模型和个体学习器的对比实验，所以我认为对我的课题有帮助的地方有一下几点：</p>
<ol>
<li>较为准确的阐述了邮件过滤系统的特性：在线性、结构性、客户性和非均匀性(当然也不是官方描述，可以做调整)。</li>
<li>集成学习在垃圾邮件过滤上的应用。</li>
<li>验证集成学习的效果时需要加上比较实验。</li>
</ol>
<h2 id="3-基于规则的垃圾邮件过滤系统设计与实现"><a href="#3-基于规则的垃圾邮件过滤系统设计与实现" class="headerlink" title="3. 基于规则的垃圾邮件过滤系统设计与实现"></a>3. 基于规则的垃圾邮件过滤系统设计与实现</h2><p>作者：郑</p>
<p>硕士论文，对于理论基础介绍的相当详尽。主要基于已有的中文垃圾邮件过滤规则集Chinese_rules.cf进行优化改进，利用神经网络训练模型，实现了一个基于规则的垃圾邮件过滤系统。遗憾的是，该文章年代久远，论文中使用的规则集已停止维护(见下图)，最后一代能否适应当下垃圾邮件的复制环境还有待实验验证。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/Papers-Reading-of-Graduation-Project/chinese_rule.png"
                      alt="chinese_rule"
                ></p>
<p>整体看来，该论文仍有值得学习的地方：</p>
<ol>
<li>只要有可持续更新的规则集，仍可利用基于规则的方法进行垃圾邮件过滤。</li>
<li>规则集不应过于复杂，否则会严重拖垮计算性能。</li>
<li>规则集能否机器生成，或将解决规则集维护成本较大的问题。</li>
</ol>
<h2 id="4-加权贝叶斯邮件过滤方法研究"><a href="#4-加权贝叶斯邮件过滤方法研究" class="headerlink" title="4. 加权贝叶斯邮件过滤方法研究"></a>4. 加权贝叶斯邮件过滤方法研究</h2><p>作者：张</p>
<p>硕士论文的传统，背景信息和基础知识较为详尽。文章介绍的模型对NBC进行了改进，分别对邮件头和邮件做了特征提取(职位特征向量)，并按一定分权值进行加权整合，得到一个加权分数来判断是否为垃圾邮件。</p>
<p>可能存在的问题和值得学习的亮点：</p>
<ol>
<li>加权思想。</li>
<li>为什么要对邮件头做与邮件体相似的特征提取？</li>
<li>将每一条原始样本转换为若干个指纹向量的特征提取方法。</li>
</ol>
<h2 id="5-结合规则过滤和内容过滤的综合型反垃圾邮件系统的研究与实现"><a href="#5-结合规则过滤和内容过滤的综合型反垃圾邮件系统的研究与实现" class="headerlink" title="5. 结合规则过滤和内容过滤的综合型反垃圾邮件系统的研究与实现"></a>5. 结合规则过滤和内容过滤的综合型反垃圾邮件系统的研究与实现</h2><p>作者：张<br>时间：2009</p>
<p>硕士论文，关于电子邮件的生命周期的解释十分详细，对后续撰写论文有较大的帮助。可能因为年代久远，对“现有技术”的分类与其他论文存在出入，不过这一个块内容仁者见仁，理清楚就行。</p>
]]></content>
      <categories>
        <category>毕设</category>
      </categories>
      <tags>
        <tag>papers reading</tag>
      </tags>
  </entry>
  <entry>
    <title>毕设杂记</title>
    <url>/2024/11/29/%E6%AF%95%E8%AE%BE%E6%9D%82%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="毕设标题的灵感"><a href="#毕设标题的灵感" class="headerlink" title="毕设标题的灵感"></a>毕设标题的灵感</h2><p>其实没啥灵感可言，导通知说要定个题先应付一下学院的流程，遂上网冲浪寻找合适的题材。因为之后打算继续读研，javaweb相关的课题对我后续套磁的帮助不大，手头上又没有资源跑深度学习，所以打算做一些小体量的机器学习。正好选题那段时间听说outlook套磁容易进垃圾邮箱里，便对 <code>垃圾邮件的识别</code> 产生了一点兴趣(希望不是给自己挖的一个大坑)。所以就在不到一个小时的时间里草率的定下了毕设的课题。</p>
<h2 id="需要了解的知识-持续扩充，缓慢解决"><a href="#需要了解的知识-持续扩充，缓慢解决" class="headerlink" title="需要了解的知识(持续扩充，缓慢解决)"></a>需要了解的知识(持续扩充，缓慢解决)</h2><p>刚开始我以为这只是一个比较简单的小课题，也就没放在心上，等两语的考试结束以后正式开始的时候，太久没有接触专业知识的陌生感，我才发现我连email是什么，send email的工作具体细节都一无所知，只能硬着头皮干了。  </p>
<ol>
<li>什么是电子邮件？电子邮件的生命周期？</li>
<li>什么是垃圾邮件？垃圾邮件的生命周期？</li>
<li>电子邮件的协议格式有哪些？具体内容是什么？</li>
<li>现在主流的垃圾邮件过滤器已经做到了什么程度？不同的模型又有哪些不足之处？现有的改进方法有哪些？</li>
<li>数据集选什么样的比较合适？</li>
<li>如何衡量一个模型的性能？</li>
</ol>
<p>个人认为解决了上述的几个问题，会对这个课题有一个较为全面的了解。这个课题并不是非常新颖，在中文期刊中都已经有相当多的研究成果出现，所以有很多的问题可以在 <strong>阅读文献</strong> 后找到答案。</p>
<h2 id="先搞清楚，电子邮件！"><a href="#先搞清楚，电子邮件！" class="headerlink" title="先搞清楚，电子邮件！"></a>先搞清楚，电子邮件！</h2><p>这部分的内容比较杂，我就放一些查到的资料把。一封能在互联网上传播的电子邮件应包含的内容：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%AF%95%E8%AE%BE%E6%9D%82%E8%AE%B0/email_sample.png"
                      alt="email_sample"
                ></p>
<p>根据 <strong>RFC 822</strong> 和 <strong>MIME</strong> 标准，一封标准的电子邮件应包括以下几个组成部分：</p>
<h3 id="1-邮件头部-Headers"><a href="#1-邮件头部-Headers" class="headerlink" title="1. 邮件头部 (Headers)"></a>1. 邮件头部 (Headers)</h3><p>邮件头部包含了关于邮件本身和邮件通信过程的元信息。常见的邮件头部字段包括：</p>
<h4 id="1-1-发件人-From"><a href="#1-1-发件人-From" class="headerlink" title="1.1 发件人 (From)"></a>1.1 发件人 (From)</h4><ul>
<li>指示邮件的发送者。</li>
<li>格式：<code>From: sender@example.com</code></li>
<li>可以包括发件人的姓名，例如：<code>From: &quot;John Doe&quot; &lt;john.doe@example.com&gt;</code></li>
</ul>
<h4 id="1-2-收件人-To"><a href="#1-2-收件人-To" class="headerlink" title="1.2 收件人 (To)"></a>1.2 收件人 (To)</h4><ul>
<li>指定邮件的主要接收者。</li>
<li>格式：<code>To: recipient@example.com</code></li>
<li>可以包括多个收件人，用逗号分隔。</li>
</ul>
<h4 id="1-3-抄送-CC-和-密送-BCC"><a href="#1-3-抄送-CC-和-密送-BCC" class="headerlink" title="1.3 抄送 (CC) 和 密送 (BCC)"></a>1.3 抄送 (CC) 和 密送 (BCC)</h4><ul>
<li><strong>抄送</strong>（CC）表示同时发送给其他人，而<strong>密送</strong>（BCC）则是发送给其他人但不公开其他收件人的地址。</li>
<li>格式：<code>CC: cc1@example.com, cc2@example.com</code></li>
<li>格式：<code>BCC: bcc@example.com</code></li>
</ul>
<h4 id="1-4-主题-Subject"><a href="#1-4-主题-Subject" class="headerlink" title="1.4 主题 (Subject)"></a>1.4 主题 (Subject)</h4><ul>
<li>邮件的主题行，简洁明了地概述邮件内容。</li>
<li>格式：<code>Subject: Meeting agenda for next week</code></li>
</ul>
<h4 id="1-5-日期-Date"><a href="#1-5-日期-Date" class="headerlink" title="1.5 日期 (Date)"></a>1.5 日期 (Date)</h4><ul>
<li>指明邮件的发送日期和时间。</li>
<li>格式：<code>Date: Wed, 21 Aug 2024 14:30:00 +0000</code></li>
</ul>
<h4 id="1-6-邮件标识-Message-ID"><a href="#1-6-邮件标识-Message-ID" class="headerlink" title="1.6 邮件标识 (Message-ID)"></a>1.6 邮件标识 (Message-ID)</h4><ul>
<li>每封邮件都会有一个唯一的标识符，帮助邮件系统跟踪邮件。</li>
<li>格式：<code>Message-ID: &lt;1234567890@example.com&gt;</code></li>
</ul>
<h4 id="1-7-回复到-Reply-To"><a href="#1-7-回复到-Reply-To" class="headerlink" title="1.7 回复到 (Reply-To)"></a>1.7 回复到 (Reply-To)</h4><ul>
<li>用于指定回复邮件时的目标地址，如果与发件人地址不同。</li>
<li>格式：<code>Reply-To: reply@example.com</code></li>
</ul>
<h4 id="1-8-其他头部字段"><a href="#1-8-其他头部字段" class="headerlink" title="1.8 其他头部字段"></a>1.8 其他头部字段</h4><ul>
<li><strong>MIME-Version</strong>：标明邮件采用的 MIME 版本，通常是 <code>MIME-Version: 1.0</code>。</li>
<li><strong>Content-Type</strong>：指定邮件内容的类型（如文本、HTML、附件等），在 MIME 邮件中非常重要。</li>
<li><strong>Content-Transfer-Encoding</strong>：描述邮件内容的编码方式（如 Base64 或 Quoted-Printable）。</li>
</ul>
<h3 id="2-邮件正文-Body"><a href="#2-邮件正文-Body" class="headerlink" title="2. 邮件正文 (Body)"></a>2. 邮件正文 (Body)</h3><p>邮件的正文是邮件的核心部分，包含具体的交流内容。正文的格式会根据 <strong>Content-Type</strong> 头部字段进行不同的处理。</p>
<h4 id="2-1-MIME-邮件的编码"><a href="#2-1-MIME-邮件的编码" class="headerlink" title="2.1 MIME 邮件的编码"></a>2.1 MIME 邮件的编码</h4><ul>
<li>如果邮件包含非 ASCII 字符或附件，它将使用 MIME（Multipurpose Internet Mail Extensions）标准进行编码，常见的编码方式有：<ul>
<li><strong>Base64</strong>：用于编码二进制数据（如图像、文档等附件）。</li>
<li><strong>Quoted-Printable</strong>：用于编码较为简单的文本数据，避免控制字符导致邮件乱码。</li>
</ul>
</li>
</ul>
<h4 id="2-2-文本邮件-Text"><a href="#2-2-文本邮件-Text" class="headerlink" title="2.2 文本邮件 (Text)"></a>2.2 文本邮件 (Text)</h4><ul>
<li><strong>纯文本</strong>（text&#x2F;plain）邮件的正文内容通常是不带格式的文本。</li>
<li><strong>格式</strong>： <code>Content-Type: text/plain; charset=&quot;UTF-8&quot;</code></li>
</ul>
<h4 id="2-3-HTML-邮件-HTML"><a href="#2-3-HTML-邮件-HTML" class="headerlink" title="2.3 HTML 邮件 (HTML)"></a>2.3 HTML 邮件 (HTML)</h4><ul>
<li><strong>HTML 格式</strong>（text&#x2F;html）邮件允许在正文中嵌入格式化文本、链接、图像等内容。</li>
<li><strong>格式</strong>： <code>Content-Type: text/html; charset=&quot;UTF-8&quot;</code></li>
</ul>
<h4 id="2-4-多部分邮件-Multipart"><a href="#2-4-多部分邮件-Multipart" class="headerlink" title="2.4 多部分邮件 (Multipart)"></a>2.4 多部分邮件 (Multipart)</h4><ul>
<li>如果邮件包含多个部分（例如，文本和附件），则会使用 <strong>multipart</strong> 类型的内容格式。</li>
<li><strong>格式</strong>：<code>Content-Type: multipart/mixed; boundary=&quot;boundary_string&quot;</code></li>
</ul>
<h3 id="3-附件-Attachments"><a href="#3-附件-Attachments" class="headerlink" title="3. 附件 (Attachments)"></a>3. 附件 (Attachments)</h3><ul>
<li>邮件可以包含附件文件，附件将通过 MIME 编码方式嵌入到邮件正文中。</li>
<li>每个附件都会有一个独立的内容描述，如 <code>Content-Type</code> 和 <code>Content-Disposition</code>。</li>
<li>示例：<div class="code-container" data-rel="Text"><figure class="iseeu highlight text"><table><tr><td class="code"><pre><span class="line">Content-Type: application/pdf; name=&quot;file.pdf&quot;</span><br><span class="line">Content-Disposition: attachment; filename=&quot;file.pdf&quot;</span><br><span class="line">Content-Transfer-Encoding: base64</span><br></pre></td></tr></table></figure></div>
由上述的种种可以看出，email的头中蕴含这丰富的信息可以用于判断邮件是否为spam，但是它和邮件体带来的信息又是不同的。</li>
</ul>
<h2 id="什么是垃圾邮件？"><a href="#什么是垃圾邮件？" class="headerlink" title="什么是垃圾邮件？"></a>什么是垃圾邮件？</h2><p>根据中国互联网协会发布的《反垃圾邮件规范》中对垃圾邮件的界定：</p>
<ol>
<li>收件人事先没有提出要求或者同意接收的广告、电子刊物、各种形式的宣传品等宣传性的电子邮件；</li>
<li>收件人无法拒收的电子邮件；</li>
<li>隐藏发件人身份、地址、标题等信息的电子邮件；</li>
<li>含有虚假的信息源、发件人、路由等信息的电子邮件。</li>
</ol>
<p>垃圾邮件又通常具备以下特征：</p>
<ol>
<li>以商业传播或恶意手段的传播。</li>
<li>使用不真实的发件人地址。</li>
<li>匿名或不明来源。</li>
<li>附有可疑的链接或附件。</li>
<li>有较多的语法错误、拼写错误以及格式不规范等问题。</li>
</ol>
<p>垃圾邮件的定义并非绝对，常是因人而异的。一封邮件对于用户A而言是垃圾邮件，对于用户B而言不一定是垃圾邮件。因此在设计垃圾邮件过滤系统时，需要学习不同用户的需求差异。也即该系统需要具备<font color='#E07250'>用户性</font>。</p>
<h2 id="电子邮件的传输协议"><a href="#电子邮件的传输协议" class="headerlink" title="电子邮件的传输协议"></a>电子邮件的传输协议</h2><p>电子邮件的发展已经经过了较长的时间，这其中演变制定出了很多高效可行的网络传输协议，包括：</p>
<h3 id="SMTP-Simple-Mail-Transfer-Protocol"><a href="#SMTP-Simple-Mail-Transfer-Protocol" class="headerlink" title="SMTP(Simple Mail Transfer Protocol)"></a>SMTP(Simple Mail Transfer Protocol)</h3><ul>
<li>SMTP 是用于电子邮件发送的协议，负责将电子邮件从发件人邮箱传输到收件人邮箱的邮件服务器。SMTP 主要用于邮件的发送和转发，而不负责存储邮件。</li>
</ul>
<h3 id="POP3-Post-Office-Protocol-3"><a href="#POP3-Post-Office-Protocol-3" class="headerlink" title="POP3(Post Office Protocol 3)"></a>POP3(Post Office Protocol 3)</h3><ul>
<li>POP3 是用于从邮件服务器接收电子邮件的协议。它允许用户下载邮件并将邮件存储在本地计算机上。POP3 的工作方式是将邮件从服务器下载后，通常会删除服务器上的邮件副本。</li>
</ul>
<h3 id="IMAP-Internet-Message-Access-Protocol"><a href="#IMAP-Internet-Message-Access-Protocol" class="headerlink" title="IMAP(Internet Message Access Protocol)"></a>IMAP(Internet Message Access Protocol)</h3><ul>
<li>IMAP 也是一种接收邮件的协议，但与 POP3 不同，IMAP 允许邮件保留在服务器上，并且支持多设备间同步。用户可以从任何设备上访问邮件，并查看邮件的实时状态（如已读、未读、标记等）</li>
</ul>
<h3 id="MIME-Multipurpose-Internet-Mail-Extensions"><a href="#MIME-Multipurpose-Internet-Mail-Extensions" class="headerlink" title="MIME(Multipurpose Internet Mail Extensions)"></a>MIME(Multipurpose Internet Mail Extensions)</h3><ul>
<li>MIME 是扩展电子邮件功能的协议，允许电子邮件发送多种类型的内容，如文本、图片、音频、视频等。MIME 是 SMTP 的一种补充，定义了如何编码多种类型的内容，以便通过电子邮件传输。</li>
</ul>
<h3 id="ESMTP-Enhanced-SMTP"><a href="#ESMTP-Enhanced-SMTP" class="headerlink" title="ESMTP(Enhanced SMTP)"></a>ESMTP(Enhanced SMTP)</h3><ul>
<li>ESMTP 是对 SMTP 协议的扩展，增加了更多的功能和特性，如身份验证（SMTP AUTH）和加密等。它向SMTP协议添加了一些可选功能，以支持现代的电子邮件服务。</li>
</ul>
<h3 id="DSN-Delivery-Status-Notification"><a href="#DSN-Delivery-Status-Notification" class="headerlink" title="DSN(Delivery Status Notification)"></a>DSN(Delivery Status Notification)</h3><ul>
<li>DSN 是一种标准机制，用于告知发件人邮件的传送状态，包括成功、延迟或失败等。它是 SMTP 协议的一部分，用于邮件传递过程中状态反馈。</li>
</ul>
<h2 id="当前主流的垃圾邮件过滤方法"><a href="#当前主流的垃圾邮件过滤方法" class="headerlink" title="当前主流的垃圾邮件过滤方法"></a>当前主流的垃圾邮件过滤方法</h2><p>就技术层面而言，当前的垃圾邮件过滤方法主要分为三大类：</p>
<ol>
<li>基于IP的过滤。<ul>
<li>黑白名单</li>
<li>服务器端控制</li>
<li>安全认证机制</li>
</ul>
</li>
<li>基于内容的过滤<ul>
<li>基于规则的方法：<ul>
<li>Ripper、决策树、Booting决策树、粗糙集</li>
</ul>
</li>
<li>基于统计的方法：<ul>
<li>Bayes、KNN、SVM、Winnow</li>
</ul>
</li>
</ul>
</li>
<li>基于行为&#x2F;SMTP协议的过滤</li>
</ol>
<p>这些方法各有优势和局限性。基于规则的内容过滤方法虽然需要专家精心制定规则，准确性较高，易于理解，但维护成本较高，且规则的公开性使得垃圾邮件发送者能够学习并规避这些规则。而基于统计学习的方法虽然生成的规则难以直观理解，但能够在一定程度上规避上述问题，因为它们是通过大量数据学习得到的“隐式规则”。</p>
<h2 id="数据集的选择"><a href="#数据集的选择" class="headerlink" title="数据集的选择"></a>数据集的选择</h2><h3 id="1-trec2006c"><a href="#1-trec2006c" class="headerlink" title="1. trec2006c"></a>1. trec2006c</h3><ul>
<li>中文邮件集，每一封邮件都由完整的邮件头和邮件体构成。</li>
</ul>
<h3 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h3><h2 id="初步实现"><a href="#初步实现" class="headerlink" title="初步实现"></a>初步实现</h2><p>如果只是单纯的实现一个基于NBC的垃圾邮件分类器的话其实是一个很简单的课题，在不考虑性能、内存的情况下，用一个极小的数据集训练的分类器加上python已经很好的集成了常见的机器学习的模型，通常数十行代码就可以搞定。接下来的代码就是一个简单的例子。</p>
<p>代码详见 <strong>en_train.py</strong>，以下是训练的结果：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">Outcome:</span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.80      1.00      0.89         8</span><br><span class="line">           1       1.00      0.71      0.83         7</span><br><span class="line"></span><br><span class="line">    accuracy                           0.87        15</span><br><span class="line">   macro avg       0.90      0.86      0.86        15</span><br><span class="line">weighted avg       0.89      0.87      0.86        15</span><br></pre></td></tr></table></figure></div>
<p>需要注意这里的数据集并非中文垃圾邮件数据集，而是一个简化的英文邮件数据集，并且只保留了邮件的正文部分。阅读文献我们会发现<font color='#FF6F91'>一封能在互联网上流通的邮件通常有：收件人(To)、抄送(CC)、密送(BCC)、主题(Subject)、正文(Body)、签名(Signature)、附件(Attachment)等数个部分构成</font>。这显然是不合理的，但是作为一个初代分类器，这样就已经非常nice了！接下来的任务就是在这基础上慢慢地做大、做强~さあ、行くぞ！</p>
<h2 id="改用更大的数据集！"><a href="#改用更大的数据集！" class="headerlink" title="改用更大的数据集！"></a>改用更大的数据集！</h2><p>上述的始め数据集只有正负例各25个，接下来来个大的<font color='	#00C5CD'><strong>trec06c</strong></font>( <a class="link"   href="https://plg.uwaterloo.ca/~gvcormac/treccorpus06/" >https://plg.uwaterloo.ca/~gvcormac/treccorpus06/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> )。该数据集一共64,620 个文件，216 个文件夹，其中正样本数21766个，负样本数42454个。而且这个数据集是一个由不带附件的原始邮件组成数据集。邮件头和邮件体有时采用了不同的编码方式，所以需要对邮件头和邮件体进行解码。比如下面列出的一封邮件：</p>
<div class="code-container" data-rel="Txt"><figure class="iseeu highlight txt"><table><tr><td class="code"><pre><span class="line">Received: from jdl.ac.cn ([159.226.42.8])</span><br><span class="line">	by spam-gw.ccert.edu.cn (MIMEDefang) with ESMTP id j7C1ceuQ019050</span><br><span class="line">	for &lt;shi@ccert.edu.cn&gt;; Sun, 14 Aug 2005 10:02:01 +0800 (CST)</span><br><span class="line">Received: (qmail 5448 invoked from network); Sun, 14 Aug 2005 02:12:48 -0000</span><br><span class="line">Received: from unknown (HELO d47db5334f2a479) (192.168.0.233)</span><br><span class="line">  by 159.226.42.8 with SMTP; Sun, 14 Aug 2005 02:12:48 -0000</span><br><span class="line">Message-ID: &lt;000b01c59ee0$a1f666b0$e900a8c0@d47db5334f2a479&gt;</span><br><span class="line">From: &quot;pan&quot; &lt;pan@jdl.ac.cn&gt;</span><br><span class="line">To: shi@ccert.edu.cn</span><br><span class="line">Subject: =?gb2312?B?ofEgzsrSu7K/zrrX2s3ytcS159Oww/uzxg==?=</span><br><span class="line">Date: Sun, 14 Aug 2005 10:16:47 +0800</span><br><span class="line">MIME-Version: 1.0</span><br><span class="line">Content-Type: text/plain;</span><br><span class="line">	charset=&quot;gb2312&quot;</span><br><span class="line">Content-Transfer-Encoding: base64</span><br><span class="line">X-Priority: 3</span><br><span class="line">X-MSMail-Priority: Normal</span><br><span class="line">X-Mailer: Microsoft Outlook Express 6.00.2800.1506</span><br><span class="line">X-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1506</span><br><span class="line"></span><br><span class="line">讲的是孔子后人的故事。一个老领导回到家乡，跟儿子感情不和，跟贪财的孙子孔为本和睦。</span><br><span class="line">老领导的弟弟魏宗万是赶马车的。</span><br><span class="line">有个洋妞大概是考察民俗的，在他们家过年。</span><br><span class="line">孔为本总想出国，被爷爷教育了。</span><br><span class="line">最后，一家人基本和解。</span><br><span class="line">顺便问另一类电影，北京青年电影制片厂的。中越战背景。一军人被介绍了一个对象，去相亲。女方是军队医院的护士，犹豫不决，总是在回忆战场上负伤的男友，好像还没死。最后</span><br><span class="line">男方表示理解，归队了。</span><br></pre></td></tr></table></figure></div>
<p>信息量非常大，在网上可以看到一些博主在训练模型时，直接将邮件头全部砍掉，只保留邮件体部分，将这个完整的数据集简化为同上的mini数据集。这怎么看都是不对的！哪怕它们跑出来的结果再好。<font color='	#00C5CD'><strong>我们要做的过滤器应该是同时考虑邮件头和邮件体两部分的内容的逻辑上完整的过滤器。</strong></font>这一部分的参考了<a href="#2%E5%8A%A0%E6%9D%83%E8%B4%9D%E5%8F%B6%E6%96%AF%E9%82%AE%E4%BB%B6%E8%BF%87%E6%BB%A4%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6">这篇</a>，所以在训练之前，需要搞清楚一封正常的电子邮件是由哪几部分构成的。</p>
]]></content>
      <categories>
        <category>毕设</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
