<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度学习——LeNet卷积神经网络初探</title>
    <url>/2024/11/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<h2 id="模型介绍："><a href="#模型介绍：" class="headerlink" title="模型介绍："></a>模型介绍：</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/p1.png"
                      alt="image"
                ></p>
<p><strong>简单介绍：</strong> 从网络结构可以看出LeNet对于现在的大模型来说是一个非常小的神经网络，他一共由7个层顺序连接组成。<font color="#845EC2">分别是卷积层、pooling层、卷积层、pooling层和三个全连接层</font>。用现代的深度学习框架来实现代码如下：  </p>
<h2 id="代码实现和解读："><a href="#代码实现和解读：" class="headerlink" title="代码实现和解读："></a>代码实现和解读：</h2><div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></div>

<p><strong>解读：</strong> 这一部分是有关网络的定义，可以看出网络的基本层实现都调用了torch的库，<code>sigmoid()</code> 函数的作用是：<font color="#845EC2">让网络中各层叠加后不会坍缩，因为引入了非线性函数。</font>我们来输出一下网络的各层的结构。</p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t:&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">Conv2d output shape: 	: torch.Size([1, 6, 28, 28])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 6, 28, 28])</span><br><span class="line">AvgPool2d output shape: : torch.Size([1, 6, 14, 14])</span><br><span class="line">Conv2d output shape: 	: torch.Size([1, 16, 10, 10])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 16, 10, 10])</span><br><span class="line">AvgPool2d output shape: : torch.Size([1, 16, 5, 5])</span><br><span class="line">Flatten output shape: 	: torch.Size([1, 400])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 120])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 120])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 84])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 84])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 10])</span><br></pre></td></tr></table></figure></div>
<p>接下来我们利用沐神的<code>d2l</code>包中的数据集准备函数来下载<code>MNIST</code>数据集。</p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):  <span class="comment"># * 判断变量的类型</span></span><br><span class="line">        net.<span class="built_in">eval</span>()          </span><br><span class="line">        <span class="comment">#? 将网络设置为评估模式, 在此模式下，net会关闭一些特定的训练技巧以确保网络的行为和训练时一致</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)  <span class="comment"># * .to(device)是为了将数据送至指定的设备上进行计算</span></span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]  <span class="comment"># * 这里返回的是预测精度</span></span><br></pre></td></tr></table></figure></div>
<p>以上的这段代码的关键步骤是<font color="#FF6F91">执行了.to(device)操作，上述方法作用的调用可用的GPU进行加速运算</font>。  </p>
<p>接下来这段代码是对net执行训练的方法定义：  </p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)  <span class="comment"># ? 初始化参数</span></span><br><span class="line">            </span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()  <span class="comment"># ? 交叉熵损失函数</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">3</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">2</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()  <span class="comment"># ? 将网络设置为训练模式</span></span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):  <span class="comment"># ? enumerate会返回索引同时返回对应迭代次数时的元素</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span> <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>]*num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span> <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

<p>这段代码非常的长，我们将其分为几个部分来进行解读：  </p>
<p><strong>首先：</strong>  </p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)  <span class="comment"># ? 初始化参数</span></span><br><span class="line">            </span><br><span class="line">    net.apply(init_weights)</span><br></pre></td></tr></table></figure></div>
<p>这一段摘要做的是网络<font color='#FFC75F'>所有参数的初始化</font>。  </p>
<p><strong>其次：</strong></p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">net.to(device)</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.CrossEntropyLoss()  <span class="comment"># ? 交叉熵损失函数</span></span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">3</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">2</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br></pre></td></tr></table></figure></div>
<p>这一段主要是<font color='#FFC75F'>定义了网络训练和结果可视化的必要变量，并将网络放在GPU上进行运行</font>。</p>
<p><strong>接下来：</strong></p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    net.train()  <span class="comment"># ? 将网络设置为训练模式</span></span><br><span class="line">    <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):  <span class="comment"># ? enumerate会返回索引同时返回对应迭代次数时的元素</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">        timer.stop()</span><br><span class="line">        train_l = metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line">        train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">            animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">    test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br></pre></td></tr></table></figure></div>
<p>这一部分是最重要的训练部分：<font color='#00C9A7'>前向传导、计算损失、对损失进行反向传导并计算梯度、根据梯度来更新参数</font>。对每一个样本都进行上述的基本过程。</p>
<p>剩下的部分就是对训练的中间过程进行适当的输出。</p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure></div>
<p>这段代码描述的是网络训练器使用的过程。根据上述参数定义，得到的训练结果如下图：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/p2.png"
                      alt="image"
                ></p>
<h2 id="模型局部最优化："><a href="#模型局部最优化：" class="headerlink" title="模型局部最优化："></a>模型局部最优化：</h2><p>接下来，我想做的是，利用循环和结果可视化来找到这个模型下的局部最优超参数。</p>
<p>–つづけ</p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>东游记(一)--此行伊始</title>
    <url>/2024/11/13/Journey-to-the-East1/</url>
    <content><![CDATA[<h2 id="写在最前"><a href="#写在最前" class="headerlink" title="写在最前"></a>写在最前</h2><p>折腾了好久，可算是把这个简陋的博客带起来了🤣。又回想起了早两年被编程环境支配的日子了。这次的博客是基于GitHub搭的，也许是感觉这样比博客园更高级？之前也没怎么接触过 <code>git</code>，就当是顺带学一下了。</p>
<p>特意开了一个新的 <code>categories</code> 用来记录 <strong>四非菜鸡如我的修士养成之路</strong> 的。哈哈，搞得好像我已经上岸开始写经验贴了一样，<font color='#FFC75F'>もう大丈夫だから、絶対に修士になるよ！</font></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/Journey-to-the-East1/p3.jpg"
                      alt="image"
                ></p>
<h2 id="为何出发"><a href="#为何出发" class="headerlink" title="为何出发"></a>为何出发</h2><p>该死的互联网留学机构，都什么年代了还在吹是个人都能润日。 </p>
<p>我想最初埋下种子是懵懂的大一，年轻时真是啥也不懂，不过从目前来看这个选择是相当正确的。时运感人，我本科班主任（下称“华子哥”）就是一个日留子。因为华子哥刚从海外回来，按照瑶湖女专的规定新任教师都要带一届本科生，同专业的其他班都配备专职辅导员，我这是什么运气😎。第一次班会华子哥就分享了自己的个人了经历，三年前的我一定做梦也想不到，三年后我将抛开所有国内可行的门路，开启东游之旅。</p>
<p>真正驱使我走上这条不归路的又是什么呢？我想，也许是那个明明很菜却又不想比任何人更烂的我在作祟，所以编织出了‘选择大于努力’的借口；也许是二〇年大家众所周知的原因，让国服考研、考公、考编成了三幻神，就业环境在两三年内断崖式崩坏；也许是当年的网络烂梗‘世界那么大 我想去看看’在搞鬼，没有出逃能力的前二十年积蓄了太多动力；也许是想再一次证明，除去高考，自己还有再去全力以赴做一件事的决心和能力。</p>
<p>我很功利，无法认可最开始就知道是毫无价值的事情，所以我无法接受国服的考研模式，润至少学会了一门语言？（政治、英语苦手你就知道我为啥接受不了了）。如果没有接触到润日，估计就是更保研狗拼刺刀或者是找工作去了。</p>
<p>另一方面，我没什么主见，保研、考研、找工作……我可能会在其中反复很跳，结果会怎么样真不太好说，所以我需要的是做减法，不给自己留下太多选项，这也是我在做选择时的一贯做法，<font color='#00C9A7'>永远搞不清自己想要什么，但是清楚的知道自己讨厌啥</font>。24年9月底保研狗上岸的时候，怎么说呢，心里不算平静吧，我的保研分应该是能够着末班车的，但我连系统都没填，也就没啥可焦虑的了嘿。但谁又能说我没有那么一瞬想着也许我本该可以休息呢？</p>
<p>我想我还是应了那句话 <code>心比天高 命比纸薄</code></p>
<h2 id="当下情况"><a href="#当下情况" class="headerlink" title="当下情况"></a>当下情况</h2><p>花了一年的时间通了两语，真是没一点语言天赋，踮起脚勉强够到出愿门槛。不过“速通”完接下来就是我比较擅长的了。相比日语，英语我是真的大苦手，至少现在微积分的日文教程能看懂个七七八八？绩点方面前面也说了我是可能有机会保研，不至于太烂，兴许得感谢我的前女友，至少前几年是读了点书的。ごめんなさい，年前的时候太不成熟，没分开说不定我现在已经躺了。往事不复提，恭喜你已经上岸，接下来就是我的主场了。</p>
<h2 id="当务之急"><a href="#当务之急" class="headerlink" title="当务之急"></a>当务之急</h2><p>等十一月的toeic结束后，就开始做毕设和套磁了，初定在年前搞定，套磁因为要套六所左右，进度没那么快，可以适当的延后。在这期间，同步复习修考的课程，数学四门左右and专业课四门左右，准备八到十门足以应对我想考的所有学校了。明年应该就是就是真正意义上的脱产备考了。<font color="#FF6F91">24年12月开始，属于我的修考之旅算是正式的开始了。</font></p>
<h2 id="暂此歇笔"><a href="#暂此歇笔" class="headerlink" title="暂此歇笔"></a>暂此歇笔</h2><p>我的古怪想法很多，但是真到输出的时候就变哑巴了。也许是因为我的灵魂还是过于干瘪，还得看更多的动漫玩更多的gal（bushi）。专栏不定期更新，少说多做，虽然我知道根本没人看，没什么期待才会偶遇惊喜嘛！じゃね、おやすみなさい～</p>
]]></content>
      <categories>
        <category>东游记</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>记录</tag>
      </tags>
  </entry>
</search>
