<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度学习——LeNet卷积神经网络初探</title>
    <url>/2024/11/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<h2 id="模型介绍："><a href="#模型介绍：" class="headerlink" title="模型介绍："></a>模型介绍：</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/p1.png"
                      alt="image"
                ></p>
<p><strong>简单介绍：</strong> 从网络结构可以看出LeNet对于现在的大模型来说是一个非常小的神经网络，他一共由7个层顺序连接组成。<font color="#845EC2">分别是卷积层、pooling层、卷积层、pooling层和三个全连接层</font>。用现代的深度学习框架来实现代码如下：  </p>
<h2 id="代码实现和解读："><a href="#代码实现和解读：" class="headerlink" title="代码实现和解读："></a>代码实现和解读：</h2><div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></div>

<p><strong>解读：</strong> 这一部分是有关网络的定义，可以看出网络的基本层实现都调用了torch的库，<code>sigmoid()</code> 函数的作用是：<font color="#845EC2">让网络中各层叠加后不会坍缩，因为引入了非线性函数。</font>我们来输出一下网络的各层的结构。</p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t:&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">Conv2d output shape: 	: torch.Size([1, 6, 28, 28])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 6, 28, 28])</span><br><span class="line">AvgPool2d output shape: : torch.Size([1, 6, 14, 14])</span><br><span class="line">Conv2d output shape: 	: torch.Size([1, 16, 10, 10])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 16, 10, 10])</span><br><span class="line">AvgPool2d output shape: : torch.Size([1, 16, 5, 5])</span><br><span class="line">Flatten output shape: 	: torch.Size([1, 400])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 120])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 120])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 84])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 84])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 10])</span><br></pre></td></tr></table></figure></div>
<p>接下来我们利用沐神的<code>d2l</code>包中的数据集准备函数来下载<code>MNIST</code>数据集。</p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):  <span class="comment"># * 判断变量的类型</span></span><br><span class="line">        net.<span class="built_in">eval</span>()          </span><br><span class="line">        <span class="comment">#? 将网络设置为评估模式, 在此模式下，net会关闭一些特定的训练技巧以确保网络的行为和训练时一致</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)  <span class="comment"># * .to(device)是为了将数据送至指定的设备上进行计算</span></span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]  <span class="comment"># * 这里返回的是预测精度</span></span><br></pre></td></tr></table></figure></div>
<p>以上的这段代码的关键步骤是<font color="#FF6F91">执行了.to(device)操作，上述方法作用的调用可用的GPU进行加速运算</font>。  </p>
<p>接下来这段代码是对net执行训练的方法定义：  </p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)  <span class="comment"># ? 初始化参数</span></span><br><span class="line">            </span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()  <span class="comment"># ? 交叉熵损失函数</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">3</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">2</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()  <span class="comment"># ? 将网络设置为训练模式</span></span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):  <span class="comment"># ? enumerate会返回索引同时返回对应迭代次数时的元素</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span> <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>]*num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span> <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

<p>这段代码非常的长，我们将其分为几个部分来进行解读：  </p>
<p><strong>首先：</strong>  </p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)  <span class="comment"># ? 初始化参数</span></span><br><span class="line">            </span><br><span class="line">    net.apply(init_weights)</span><br></pre></td></tr></table></figure></div>
<p>这一段摘要做的是网络<font color='#FFC75F'>所有参数的初始化</font>。  </p>
<p><strong>其次：</strong></p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">net.to(device)</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.CrossEntropyLoss()  <span class="comment"># ? 交叉熵损失函数</span></span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">3</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">2</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br></pre></td></tr></table></figure></div>
<p>这一段主要是<font color='#FFC75F'>定义了网络训练和结果可视化的必要变量，并将网络放在GPU上进行运行</font>。</p>
<p><strong>接下来：</strong></p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    net.train()  <span class="comment"># ? 将网络设置为训练模式</span></span><br><span class="line">    <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):  <span class="comment"># ? enumerate会返回索引同时返回对应迭代次数时的元素</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">        timer.stop()</span><br><span class="line">        train_l = metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line">        train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">            animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">    test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br></pre></td></tr></table></figure></div>
<p>这一部分是最重要的训练部分：<font color='#00C9A7'>前向传导、计算损失、对损失进行反向传导并计算梯度、根据梯度来更新参数</font>。对每一个样本都进行上述的基本过程。</p>
<p>剩下的部分就是对训练的中间过程进行适当的输出。</p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure></div>
<p>这段代码描述的是网络训练器使用的过程。根据上述参数定义，得到的训练结果如下图：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/p2.png"
                      alt="image"
                ></p>
<h2 id="模型局部最优化："><a href="#模型局部最优化：" class="headerlink" title="模型局部最优化："></a>模型局部最优化：</h2><p>接下来，我想做的是，利用循环和结果可视化来找到这个模型下的局部最优超参数。</p>
<p>–つづけ</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>test</title>
    <url>/2024/11/13/test/</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
