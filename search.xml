<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度学习——LeNet卷积神经网络初探</title>
    <url>/2024/11/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<h2 id="模型介绍："><a href="#模型介绍：" class="headerlink" title="模型介绍："></a>模型介绍：</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/p1.png"
                      alt="image"
                ></p>
<p><strong>简单介绍：</strong> 从网络结构可以看出LeNet对于现在的大模型来说是一个非常小的神经网络，他一共由7个层顺序连接组成。<font color="#845EC2">分别是卷积层、pooling层、卷积层、pooling层和三个全连接层</font>。用现代的深度学习框架来实现代码如下：  </p>
<h2 id="代码实现和解读："><a href="#代码实现和解读：" class="headerlink" title="代码实现和解读："></a>代码实现和解读：</h2><div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></div>

<p><strong>解读：</strong> 这一部分是有关网络的定义，可以看出网络的基本层实现都调用了torch的库，<code>sigmoid()</code> 函数的作用是：<font color="#845EC2">让网络中各层叠加后不会坍缩，因为引入了非线性函数。</font>我们来输出一下网络的各层的结构。</p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t:&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure></div>

<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">Conv2d output shape: 	: torch.Size([1, 6, 28, 28])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 6, 28, 28])</span><br><span class="line">AvgPool2d output shape: : torch.Size([1, 6, 14, 14])</span><br><span class="line">Conv2d output shape: 	: torch.Size([1, 16, 10, 10])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 16, 10, 10])</span><br><span class="line">AvgPool2d output shape: : torch.Size([1, 16, 5, 5])</span><br><span class="line">Flatten output shape: 	: torch.Size([1, 400])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 120])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 120])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 84])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 84])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 10])</span><br></pre></td></tr></table></figure></div>
<p>接下来我们利用沐神的<code>d2l</code>包中的数据集准备函数来下载<code>MNIST</code>数据集。</p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):  <span class="comment"># * 判断变量的类型</span></span><br><span class="line">        net.<span class="built_in">eval</span>()          </span><br><span class="line">        <span class="comment">#? 将网络设置为评估模式, 在此模式下，net会关闭一些特定的训练技巧以确保网络的行为和训练时一致</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)  <span class="comment"># * .to(device)是为了将数据送至指定的设备上进行计算</span></span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]  <span class="comment"># * 这里返回的是预测精度</span></span><br></pre></td></tr></table></figure></div>
<p>以上的这段代码的关键步骤是<font color="#FF6F91">执行了.to(device)操作，上述方法作用的调用可用的GPU进行加速运算</font>。  </p>
<p>接下来这段代码是对net执行训练的方法定义：  </p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)  <span class="comment"># ? 初始化参数</span></span><br><span class="line">            </span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()  <span class="comment"># ? 交叉熵损失函数</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">3</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">2</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()  <span class="comment"># ? 将网络设置为训练模式</span></span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):  <span class="comment"># ? enumerate会返回索引同时返回对应迭代次数时的元素</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span> <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>]*num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span> <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

<p>这段代码非常的长，我们将其分为几个部分来进行解读：  </p>
<p><strong>首先：</strong>  </p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)  <span class="comment"># ? 初始化参数</span></span><br><span class="line">            </span><br><span class="line">    net.apply(init_weights)</span><br></pre></td></tr></table></figure></div>
<p>这一段摘要做的是网络<font color='#FFC75F'>所有参数的初始化</font>。  </p>
<p><strong>其次：</strong></p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">net.to(device)</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.CrossEntropyLoss()  <span class="comment"># ? 交叉熵损失函数</span></span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">3</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">2</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br></pre></td></tr></table></figure></div>
<p>这一段主要是<font color='#FFC75F'>定义了网络训练和结果可视化的必要变量，并将网络放在GPU上进行运行</font>。</p>
<p><strong>接下来：</strong></p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    net.train()  <span class="comment"># ? 将网络设置为训练模式</span></span><br><span class="line">    <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):  <span class="comment"># ? enumerate会返回索引同时返回对应迭代次数时的元素</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">        timer.stop()</span><br><span class="line">        train_l = metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line">        train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">            animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">    test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br></pre></td></tr></table></figure></div>
<p>这一部分是最重要的训练部分：<font color='#00C9A7'>前向传导、计算损失、对损失进行反向传导并计算梯度、根据梯度来更新参数</font>。对每一个样本都进行上述的基本过程。</p>
<p>剩下的部分就是对训练的中间过程进行适当的输出。</p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure></div>
<p>这段代码描述的是网络训练器使用的过程。根据上述参数定义，得到的训练结果如下图：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/p2.png"
                      alt="image"
                ></p>
<h2 id="模型局部最优化："><a href="#模型局部最优化：" class="headerlink" title="模型局部最优化："></a>模型局部最优化：</h2><p>接下来，我想做的是，利用循环和结果可视化来找到这个模型下的局部最优超参数。</p>
<p>–つづけ</p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>CNN</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>东游记(一)--此行伊始</title>
    <url>/2024/11/13/Journey-to-the-East1/</url>
    <content><![CDATA[<h2 id="写在最前"><a href="#写在最前" class="headerlink" title="写在最前"></a>写在最前</h2><p>折腾了好久，可算是把这个简陋的博客带起来了🤣。又回想起了早两年被编程环境支配的日子了。这次的博客是基于GitHub搭的，也许是感觉这样比博客园更高级？之前也没怎么接触过 <code>git</code>，就当是顺带学一下了。</p>
<p>特意开了一个新的 <code>categories</code> 用来记录 <strong>四非菜鸡如我的修士养成之路</strong> 的。哈哈，搞得好像我已经上岸开始写经验贴了一样，<font color='#FFC75F'>もう大丈夫だから、絶対に修士になるよ！</font></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/Journey-to-the-East1/p3.jpg"
                      alt="image"
                ></p>
<h2 id="为何出发"><a href="#为何出发" class="headerlink" title="为何出发"></a>为何出发</h2><p>该死的互联网留学机构，都什么年代了还在吹是个人都能润日。 </p>
<p>我想最初埋下种子是懵懂的大一，年轻时真是啥也不懂，不过从目前来看这个选择是相当正确的。时运感人，我本科班主任（下称“华子哥”）就是一个日留子。因为华子哥刚从海外回来，按照瑶湖女专的规定新任教师都要带一届本科生，同专业的其他班都配备专职辅导员，我这是什么运气😎。第一次班会华子哥就分享了自己的个人了经历，三年前的我一定做梦也想不到，三年后我将抛开所有国内可行的门路，开启东游之旅。</p>
<p>真正驱使我走上这条不归路的又是什么呢？我想，也许是那个明明很菜却又不想比任何人更烂的我在作祟，所以编织出了‘选择大于努力’的借口；也许是二〇年大家众所周知的原因，让国服考研、考公、考编成了三幻神，就业环境在两三年内断崖式崩坏；也许是当年的网络烂梗‘世界那么大 我想去看看’在搞鬼，没有出逃能力的前二十年积蓄了太多动力；也许是想再一次证明，除去高考，自己还有再去全力以赴做一件事的决心和能力。</p>
<p>我很功利，无法认可最开始就知道是毫无价值的事情，所以我无法接受国服的考研模式，润至少学会了一门语言？（政治、英语苦手你就知道我为啥接受不了了）。如果没有接触到润日，估计就是更保研狗拼刺刀或者是找工作去了。</p>
<p>另一方面，我没什么主见，保研、考研、找工作……我可能会在其中反复很跳，结果会怎么样真不太好说，所以我需要的是做减法，不给自己留下太多选项，这也是我在做选择时的一贯做法，<font color='#00C9A7'>永远搞不清自己想要什么，但是清楚的知道自己讨厌啥</font>。24年9月底保研狗上岸的时候，怎么说呢，心里不算平静吧，我的保研分应该是能够着末班车的，但我连系统都没填，也就没啥可焦虑的了嘿。但谁又能说我没有那么一瞬想着也许我本该可以休息呢？</p>
<p>我想我还是应了那句话 <code>心比天高 命比纸薄</code></p>
<h2 id="当下情况"><a href="#当下情况" class="headerlink" title="当下情况"></a>当下情况</h2><p>花了一年的时间通了两语，真是没一点语言天赋，踮起脚勉强够到出愿门槛。不过“速通”完接下来就是我比较擅长的了。相比日语，英语我是真的大苦手，至少现在微积分的日文教程能看懂个七七八八？绩点方面前面也说了我是可能有机会保研，不至于太烂，兴许得感谢我的前女友，至少前几年是读了点书的。ごめんなさい，年前的时候太不成熟，没分开说不定我现在已经躺了。往事不复提，恭喜你已经上岸，接下来就是我的主场了。</p>
<h2 id="当务之急"><a href="#当务之急" class="headerlink" title="当务之急"></a>当务之急</h2><p>等十一月的toeic结束后，就开始做毕设和套磁了，初定在年前搞定，套磁因为要套六所左右，进度没那么快，可以适当的延后。在这期间，同步复习修考的课程，数学四门左右and专业课四门左右，准备八到十门足以应对我想考的所有学校了。明年应该就是就是真正意义上的脱产备考了。<font color="#FF6F91">24年12月开始，属于我的修考之旅算是正式的开始了。</font></p>
<h2 id="暂此歇笔"><a href="#暂此歇笔" class="headerlink" title="暂此歇笔"></a>暂此歇笔</h2><p>我的古怪想法很多，但是真到输出的时候就变哑巴了。也许是因为我的灵魂还是过于干瘪，还得看更多的动漫玩更多的gal（bushi）。专栏不定期更新，少说多做，虽然我知道根本没人看，没什么期待才会偶遇惊喜嘛！じゃね、おやすみなさい～</p>
]]></content>
      <categories>
        <category>东游记</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>记录</tag>
      </tags>
  </entry>
  <entry>
    <title>git command</title>
    <url>/2024/12/01/git-command/</url>
    <content><![CDATA[<p><strong>一下全部内容均搬运于b站up“GeekHour”，仅个人学习使用</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/git-command/Git.png"
                      alt="git command"
                ></p>
]]></content>
      <categories>
        <category>常用命令</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title>Experience in improving spoken Japanese</title>
    <url>/2024/12/02/Experience-in-improving-spoken-Japanese/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>斥巨资5r买来的经验贴，感觉很实用所以整理好了放在自己的blog上，期待有缘人发现，当然如果有兴趣提升日语口语的欢迎和我talk😉😉😉🤩🤩🤩！</p>
<p>感觉提升一门语言听说的最快方法就是创造语境，这份经验贴也是如此，跟着下面的步骤走在国内也可以有语境，但大大大前提是要<code>坚持</code>下来。</p>
<h2 id="第一阶段：掌握单词、短句发音"><a href="#第一阶段：掌握单词、短句发音" class="headerlink" title="第一阶段：掌握单词、短句发音"></a>第一阶段：掌握单词、短句发音</h2><p>推荐时间:2个月<br>b站明王道跟动漫学标日单词<br>【【单词速记】跟动漫学新标日单词(初级上册合集)-哔哩哔哩】<a class="link"   href="https://b23.tv/tEkVa30" >https://b23.tv/tEkVa30 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>  </p>
<p>复述每句动漫台词，每天花一个半小时左右，直到把每个发音都听清</p>
<p>因为我在得到这份经验贴的时候刚过完N2，单词的发音这一块还有点印象的，所以这一块不会花太长的时间。但还是要过一遍的，毕竟基础不牢地动山摇。</p>
<h2 id="第二阶段：熟练不同场景下的对话"><a href="#第二阶段：熟练不同场景下的对话" class="headerlink" title="第二阶段：熟练不同场景下的对话"></a>第二阶段：熟练不同场景下的对话</h2><p>推荐时间：半年<br>最最剧场APP  </p>
<p>要求：必须选择1分钟左右动漫片段，台词十几句的进行配音(不要选择其他素材，后期用的时候动漫特定场景应用会浮现在脑海里，别的素材做不到) </p>
<p>数量：保质保量完成50个作品(每个作品练习时长至少半小时)  </p>
<p>听力巩固：每天听自己配过的作品，最好一个作品看几百遍</p>
<h2 id="第三阶段：日常对话训练"><a href="#第三阶段：日常对话训练" class="headerlink" title="第三阶段：日常对话训练"></a>第三阶段：日常对话训练</h2><p>推荐时间：不在日本的所有时间<br>Hellotalk APP  </p>
<p>找陌生日本人搭话，先文字聊天(学会使用日语输入法，聊几天就有组织句子的感觉了)，然后找人约打电话，最好对方会一些中文，找固定语伴，每周一小时，对话可达日常水平</p>
<h2 id="总结：无他，但手熟尔"><a href="#总结：无他，但手熟尔" class="headerlink" title="总结：无他，但手熟尔"></a>总结：无他，但手熟尔</h2><p>不必留于形式，看完这篇经验贴就把他忘了吧，你需要做的是花更多的时间去记单词、去组织语言、去交流。</p>
<p>日语学习中常会听到一句话<code>“N1过了就算入门了”</code>，是也是这个理，有没有这个N1都无所谓，大胆开口去说就完了！</p>
]]></content>
      <tags>
        <tag>日语</tag>
        <tag>经验贴</tag>
      </tags>
  </entry>
  <entry>
    <title>conda_create</title>
    <url>/2024/12/08/conda-create/</url>
    <content><![CDATA[<h2 id="conda下载"><a href="#conda下载" class="headerlink" title="conda下载"></a>conda下载</h2><p>conda下载没什么需要注意的，也不用按着啥教程来，直接去官网上下载就行，不过这里处于节约内存的考虑，我下载的是 <code>miniconda</code>。相较于anaconda的满血版，miniconda更为轻量级，因为主环境一般也不方便使用(可能是我不会用？)，而其他不同版本的环境下的各个包又是相互独立的，所以个人感觉miniconda就已经够用了，毕竟刚重装系统，内存还是要合理规划使用的。</p>
<p>刚开始下载的时候不知道咋回事下成了anaconda，可能是因为下载链接过于隐秘？这里附上miniconda的下载链接：<a class="link"   href="https://repo.anaconda.com/miniconda/" >https://repo.anaconda.com/miniconda/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> 。除了配置一下安装的目录位置以及按需将conda加入到PATH环境中，其他的就是一路Enter了。</p>
<h2 id="创建一个新的conda环境"><a href="#创建一个新的conda环境" class="headerlink" title="创建一个新的conda环境"></a>创建一个新的conda环境</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>创建环境一般需要指定python的版本，好像不指定就是默认？秉持着用双数版本不用最新的原则，我打算创建一个3.10.x版本的conda环境。初始化的命令如下；</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda create --name &lt;Env_name&gt; python=&lt;version&gt;</span><br></pre></td></tr></table></figure></div>

<p>一般用梯子创建不会有啥问题，这一步到这里就完成了。</p>
<h3 id="下载必要的包"><a href="#下载必要的包" class="headerlink" title="下载必要的包"></a>下载必要的包</h3><p>初始化之后可以在终端看到我们仍然在base环境下，想要进入新的环境需要<font color="#FF6F91">激活它</font>。激活的命令如下：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">active &lt;Env_name&gt;</span><br></pre></td></tr></table></figure></div>

<p>如果太久没有使用而忘记了Env的名字时，可以直接去miniconda的目录下查找，也可以用如下的命令来<font color="#FF6F91">查询已有的环境</font>：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda env list</span><br></pre></td></tr></table></figure></div>

<p>有激活当然就有<font color="#FF6F91">退出命令</font>，虽然我一般直接关闭终端根本用不上这个命令，但还是学一个吧：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure></div>

<p>接下来就按需下载必要的包就可以了，下载包一般会分为pip下载和conda下载，通常情况两种方法都是可以，但是有时只有pip有用，不过也有一些情况用pip下载的包会和其他的包冲突，欸！<font color="#845EC2">环境配置中的包冲突问题一直都是一个玄学问题</font>。下载包的命令格式如下：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda install &lt;package_name&gt;</span><br><span class="line"></span><br><span class="line">pip install &lt;package_name&gt;</span><br></pre></td></tr></table></figure></div>

<p>下载包的时候，常常会出现网络问题，毕竟人家的服务器在海外，如果遇到这种情况可以考虑使用<font color="#FF6F91">镜像地址</font>下载，只需要在后边加上如下的命令就可以了：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda install &lt;package_name&gt; -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br></pre></td></tr></table></figure></div>

<p>检查包是否下载也是某些情况下必要的操作，这里也附上代码：</p>
<div class="code-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="code"><pre><span class="line">conda list</span><br><span class="line"></span><br><span class="line">pip list</span><br><span class="line"></span><br><span class="line">conda list &lt;package_name&gt;</span><br><span class="line"></span><br><span class="line">pip list &lt;package_name&gt;</span><br></pre></td></tr></table></figure></div>]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>conda</tag>
        <tag>terminal</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Graduation Project--Papers Reading</title>
    <url>/2024/12/25/Papers-Reading-of-Graduation-Project/</url>
    <content><![CDATA[<h2 id="前言杂谈"><a href="#前言杂谈" class="headerlink" title="前言杂谈"></a>前言杂谈</h2>]]></content>
  </entry>
  <entry>
    <title>毕设杂记</title>
    <url>/2024/11/29/%E6%AF%95%E8%AE%BE%E6%9D%82%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="毕设标题的灵感"><a href="#毕设标题的灵感" class="headerlink" title="毕设标题的灵感"></a>毕设标题的灵感</h2><p>其实没啥灵感可言，导通知说要定个题先应付一下学院的流程，遂上网冲浪寻找合适的题材。因为之后打算继续读研，javaweb相关的课题对我后续套磁的帮助不大，手头上又没有资源跑深度学习，所以打算做一些小体量的机器学习。正好选题那段时间听说outlook套磁容易进垃圾邮箱里，便对 <code>垃圾邮件的识别</code> 产生了一点兴趣(希望不是给自己挖的一个大坑)。所以就在不到一个小时的时间里草率的定下了毕设的课题。</p>
<h2 id="需要了解的知识-持续扩充，缓慢解决"><a href="#需要了解的知识-持续扩充，缓慢解决" class="headerlink" title="需要了解的知识(持续扩充，缓慢解决)"></a>需要了解的知识(持续扩充，缓慢解决)</h2><p>刚开始我以为这只是一个比较简单的小课题，也就没放在心上，等两语的考试结束以后正式开始的时候，太久没有接触专业知识的陌生感，我才发现我连email是什么，send email的工作具体细节都一无所知，只能硬着头皮干了。  </p>
<ol>
<li>什么是电子邮件？电子邮件的生命周期？</li>
<li>什么是垃圾邮件？垃圾邮件的生命周期？</li>
<li>电子邮件的协议格式有哪些？具体内容是什么？</li>
<li>现在主流的垃圾邮件过滤器已经做到了什么程度？不同的模型又有哪些不足之处？现有的改进方法有哪些？</li>
<li>数据集选什么样的比较合适？</li>
</ol>
<p>个人认为解决了上述的几个问题，会对这个课题有一个较为全面的了解。这个课题并不是非常新颖，在中文期刊中都已经有相当多的研究成果出现，所以有很多的问题可以在 <strong>阅读文献</strong> 后找到答案。</p>
<h2 id="文献阅读"><a href="#文献阅读" class="headerlink" title="文献阅读"></a>文献阅读</h2><h3 id="1-《个性化垃圾邮件过滤的扩展研究》"><a href="#1-《个性化垃圾邮件过滤的扩展研究》" class="headerlink" title="1.《个性化垃圾邮件过滤的扩展研究》"></a>1.《个性化垃圾邮件过滤的扩展研究》</h3><p>作者：徐丹丹，南京航空航天大学</p>
<p>是一篇硕士论文，在文章的绪论部分对上述的若干问题有较为详尽的解释，其中对于<code>垃圾邮件的生命周期</code>, <code>电子邮件的生命周期</code>的概念解释的较为清晰，结合GPT内容整理如下：</p>
<div class="code-container" data-rel="M"><figure class="iseeu highlight m"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>. 垃圾邮件的生命周期</span><br><span class="line">垃圾邮件是指未经请求发送给大量用户的广告或恶意邮件，通常用于推广产品、服务或执行恶意活动（如传播病毒、钓鱼攻击等）。垃圾邮件的生命周期包括几个阶段：</span><br><span class="line">    <span class="number">1.1</span> 域名注册阶段</span><br><span class="line">    攻击者选择域名：垃圾邮件发送者首先会选择并购买一个域名。这个域名通常看起来合法，但它们可能是用来建立一个虚假的网站，或者用于伪装成合法的公司或服务。通过注册域名，攻击者为垃圾邮件提供了一个发送的“地址”。</span><br><span class="line">    使用虚假信息：为了掩盖身份，垃圾邮件发送者可能会使用假信息注册域名，从而使追踪和防范变得更加困难。</span><br><span class="line">    <span class="number">1.2</span> DNS 名称解析和托管</span><br><span class="line">    DNS 配置：一旦攻击者注册了域名，他们需要配置DNS（域名系统）以便正确解析域名到特定的IP地址。DNS 服务器将负责将域名转换为计算机可识别的IP地址，指向一个Web 服务器。</span><br><span class="line">    网站托管：垃圾邮件发送者会搭建一个网站，该网站可能是一个恶意网站或虚假广告网站。这个网站可能包含恶意软件，或通过诱导用户点击链接进行钓鱼攻击。</span><br><span class="line">    <span class="number">1.3</span> 垃圾邮件的发送</span><br><span class="line">    僵尸网络：垃圾邮件的发送通常依赖于僵尸网络（botnets）。僵尸网络由大量受感染的计算机组成，攻击者通过它们来分发垃圾邮件。这些计算机通常是被恶意软件感染的，它们的所有者并不知道自己计算机的“角色”。</span><br><span class="line">    发送垃圾邮件：攻击者通过这些受控的计算机发送大量垃圾邮件，目的是让受害者点击邮件中的链接，访问恶意网站或下载恶意文件。</span><br><span class="line">    <span class="number">1.4</span> 诱使用户点击链接</span><br><span class="line">    钓鱼攻击：垃圾邮件中常常会嵌入恶意链接，这些链接指向伪装成合法网站的页面，目的是窃取用户个人信息、信用卡信息等。</span><br><span class="line">    诱导点击：通过各种手段（如假冒银行邮件、虚假优惠信息等），攻击者试图诱导用户点击邮件中的链接。</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>. 电子邮件的生命周期</span><br><span class="line">电子邮件的生命周期描述了从邮件创建到最终传递或丢弃的全过程。它通常涉及以下几个主要阶段：</span><br><span class="line">    <span class="number">2.1</span> 邮件创建</span><br><span class="line">    编写邮件：用户在电子邮件客户端（如Outlook、Gmail等）中编写一封邮件。邮件包括收件人、发件人、主题、正文内容以及附件。</span><br><span class="line">    发送邮件：用户点击发送按钮后，邮件通过邮件客户端发送到邮件服务器。</span><br><span class="line">    <span class="number">2.2</span> 邮件传输</span><br><span class="line">    发件人邮件服务器：邮件首先到达发件人的邮件服务器（通常是SMTP服务器），这是电子邮件传输的起点。</span><br><span class="line">    邮件路由：邮件服务器通过DNS查找目标邮件服务器的IP地址，并根据邮件的目的地进行路由。邮件可能经过多个中转服务器，特别是在不同域名之间发送邮件时。</span><br><span class="line">    中继服务器：如果目标邮件服务器不可用，邮件会通过中继服务器转发，直到到达最终目标服务器。</span><br><span class="line">    <span class="number">2.3</span> 邮件接收</span><br><span class="line">    接收方邮件服务器：邮件最终到达接收方的邮件服务器（通常是POP3或IMAP服务器）。接收方邮件服务器负责存储和管理邮件。</span><br><span class="line">    反垃圾邮件过滤：接收方邮件服务器会进行垃圾邮件过滤，检查邮件是否符合垃圾邮件的特征。如果是垃圾邮件，邮件会被拦截并放入垃圾邮件文件夹。</span><br><span class="line">    <span class="number">2.4</span> 邮件阅读</span><br><span class="line">    接收用户读取邮件：邮件最终到达接收方的电子邮件客户端，用户可以查看邮件内容。此时，邮件可能已经被标记为“已读”。</span><br><span class="line">    邮件响应：如果邮件是正常邮件，接收方可能会回复、转发邮件，或执行其他操作。如果邮件是垃圾邮件，接收方可能会选择标记为垃圾邮件或删除。</span><br><span class="line">    <span class="number">2.5</span> 邮件存档或删除</span><br><span class="line">    存档邮件：用户可以将重要邮件存档，以便以后查阅。</span><br><span class="line">    删除邮件：不再需要的邮件会被删除，垃圾邮件通常在这一阶段被清除。</span><br><span class="line">    <span class="number">2.6</span> 邮件丢失或失败传递</span><br><span class="line">    邮件失败传递：如果由于网络故障或邮件服务器问题，邮件无法送达目标地址，邮件系统会发送“传递失败”通知。</span><br><span class="line">    丢失邮件：在极少数情况下，由于邮件系统故障或配置错误，邮件可能在传输过程中丢失。</span><br></pre></td></tr></table></figure></div>

<p>此外，该文章还提出了<code>用户个性化过滤方案</code>，在传统的通用过滤器的筛选结果的基础上，基于多任务学习原理设计了两个相互学习的过滤器分别用于收件箱和垃圾箱的邮件的再过滤。这样做可以修正第一次过滤的结果，解决类不平均问题和用户对垃圾邮件的定义差异问题。</p>
<br>

<p>还有一个我比较在一的点，<code>概念漂移</code>，<code>广义虚漂移</code>，这个概念在目前看到的几篇文献中并未被提出。GPT给出的解释是：<font color='#FF6F91'>模型在初期训练时可能能够很好地拟合数据，但随着环境的动态变化，原本用于训练的数据的特征和标签之间的关系可能不再适用，从而导致模型的性能下降。</font>我的通俗的理解就是，因为我们要解决的问题是一个会随着时间变化的问题，用固定的数据集跑出来的模型它学习到的‘概念’是固定的，所以随着时间的推移，慢慢的就会变得不那么适用。这也是因为机器学习的 <code>强假设前提</code> ：数据的统计性质（均值、方差）是保持不变的。但实际上，统计性质是可能会随着时间发生波动的。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%AF%95%E8%AE%BE%E6%9D%82%E8%AE%B0/%E6%A6%82%E5%BF%B5%E6%BC%82%E7%A7%BB.png"
                      alt="概念漂移"
                ></p>
<p>紧接着又提出了一个急需解决的问题：<code>类不平衡问题</code>，不过我感觉对于<code>二分类问题</code> 这种情况还是比较常见的，正负样本的比例不是1：1都算不平衡？感觉只要不是很严重的偏离，比如1：100，可以暂时先不管它。</p>
<h3 id="2-加权贝叶斯邮件过滤方法研究"><a href="#2-加权贝叶斯邮件过滤方法研究" class="headerlink" title="2.加权贝叶斯邮件过滤方法研究"></a>2.加权贝叶斯邮件过滤方法研究</h3><p>作者：张远，哈尔滨工业大学</p>
<p>同样是一篇硕士论文，这篇文章主要给我带来了<code>指纹向量特征编码</code>、<code>加权</code>等几个非常吸引我的点。回头看看能不能把他实现出来。</p>
<p>(24-12-18)回头了，现在看来这篇的做法好像有一点超前了，虽然说是用到了加权思想，但其实我是没有看懂ta怎么用的加权的，对邮件头做和邮件体一样的特征提取的话，感觉就破坏了他的结构了。我认为对邮件头的处理可能使用 <strong>基于规则的过滤方法</strong> 会更合理一些，但是可行性还有待验证。</p>
<p>(24-12-18)还有就是，<font color='#ac7c26'>这篇文章在处理指纹向量特征编码时，将文档切割成6-8个字符的小块</font>。串长越长计算指纹向量的时间也就越久，所以理论上来说是可以显著的提高运算效率的，但是这样操作会不会破坏文档内容的连续性作者并没有验证，也就是这样操作的合理性没有得到充分的证实，所以这个做法还有有待考究的。</p>
<h3 id="3-基于规则的垃圾邮件过滤算法比较研究"><a href="#3-基于规则的垃圾邮件过滤算法比较研究" class="headerlink" title="3.基于规则的垃圾邮件过滤算法比较研究"></a>3.基于规则的垃圾邮件过滤算法比较研究</h3><p>这是一篇学报，篇幅较短，主要就是告诉我们干了一件事情得到了什么结果，其他的细节大家自己猜吧！据作者说，ta们在<strong>WEKA平台</strong>上基于PUI预料库比较了<code>Ripper算法</code>， <code>C4.5决策树</code>， <code>AdaBoost算法</code>等三个算法的性能，发现<code>AdaBoost算法</code>的性能最好。<font color='#E07250'>ta们用的评价指标是<code>准确率Precison</code>和<code>召回率Recall</code>。</font></p>
<h2 id="初步实现"><a href="#初步实现" class="headerlink" title="初步实现"></a>初步实现</h2><p>如果只是单纯的实现一个基于NBC的垃圾邮件分类器的话其实是一个很简单的课题，在不考虑性能、内存的情况下，用一个极小的数据集训练的分类器加上python已经很好的集成了常见的机器学习的模型，通常数十行代码就可以搞定。接下来的代码就是一个简单的例子。</p>
<div class="code-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集的地址</span></span><br><span class="line">spam_dir = <span class="string">&#x27;./date2/spam/&#x27;</span></span><br><span class="line">ham_dir = <span class="string">&#x27;./date2/ham/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于存储数据和标签，标签和内容是一一对应的</span></span><br><span class="line">data = []</span><br><span class="line">labels = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># read datasets</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_data</span>(<span class="params">spam_dir, ham_dir</span>):</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(spam_dir):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join (spam_dir, filename), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            content = f.read().strip()</span><br><span class="line">            data.append(content)</span><br><span class="line">            labels.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(ham_dir):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join (ham_dir, filename), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            content = f.read().strip()</span><br><span class="line">            data.append(content)</span><br><span class="line">            labels.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 英文分词器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">english_tokenizer</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(word_tokenize(text))</span><br><span class="line"></span><br><span class="line">read_data(spam_dir, ham_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用TF-IDF向量化文本</span></span><br><span class="line">vectorizer = TfidfVectorizer(tokenizer=english_tokenizer, stop_words=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将文本转化为TF-IDF矩阵</span></span><br><span class="line">X = vectorizer.fit_transform(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签转换为数组</span></span><br><span class="line">y = labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用朴素贝叶斯分类器训练模型</span></span><br><span class="line">classifier = MultinomialNB()</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = classifier.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印分类报告</span></span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure></div>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">Outcome:</span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.80      1.00      0.89         8</span><br><span class="line">           1       1.00      0.71      0.83         7</span><br><span class="line"></span><br><span class="line">    accuracy                           0.87        15</span><br><span class="line">   macro avg       0.90      0.86      0.86        15</span><br><span class="line">weighted avg       0.89      0.87      0.86        15</span><br></pre></td></tr></table></figure></div>
<p>需要注意这里的数据集并非中文垃圾邮件数据集，而是一个简化的英文邮件数据集，并且只保留了邮件的正文部分。阅读文献我们会发现<font color='#FF6F91'>一封能在互联网上流通的邮件通常有：收件人(To)、抄送(CC)、密送(BCC)、主题(Subject)、正文(Body)、签名(Signature)、附件(Attachment)等数个部分构成</font>。这显然是不合理的，但是作为一个初代分类器，这样就已经非常nice了！接下来的任务就是在这基础上慢慢地做大、做强~さあ、行くぞ！</p>
<h2 id="改用更大的数据集！"><a href="#改用更大的数据集！" class="headerlink" title="改用更大的数据集！"></a>改用更大的数据集！</h2><p>上述的始め数据集只有正负例各25个，接下来来个大的<font color='	#00C5CD'><strong>trec06c</strong></font>( <a class="link"   href="https://plg.uwaterloo.ca/~gvcormac/treccorpus06/" >https://plg.uwaterloo.ca/~gvcormac/treccorpus06/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> )。该数据集一共64,620 个文件，216 个文件夹，其中正样本数21766个，负样本数42454个。而且这个数据集是一个由不带附件的原始邮件组成数据集。邮件头和邮件体有时采用了不同的编码方式，所以需要对邮件头和邮件体进行解码。比如下面列出的一封邮件：</p>
<div class="code-container" data-rel="Txt"><figure class="iseeu highlight txt"><table><tr><td class="code"><pre><span class="line">Received: from jdl.ac.cn ([159.226.42.8])</span><br><span class="line">	by spam-gw.ccert.edu.cn (MIMEDefang) with ESMTP id j7C1ceuQ019050</span><br><span class="line">	for &lt;shi@ccert.edu.cn&gt;; Sun, 14 Aug 2005 10:02:01 +0800 (CST)</span><br><span class="line">Received: (qmail 5448 invoked from network); Sun, 14 Aug 2005 02:12:48 -0000</span><br><span class="line">Received: from unknown (HELO d47db5334f2a479) (192.168.0.233)</span><br><span class="line">  by 159.226.42.8 with SMTP; Sun, 14 Aug 2005 02:12:48 -0000</span><br><span class="line">Message-ID: &lt;000b01c59ee0$a1f666b0$e900a8c0@d47db5334f2a479&gt;</span><br><span class="line">From: &quot;pan&quot; &lt;pan@jdl.ac.cn&gt;</span><br><span class="line">To: shi@ccert.edu.cn</span><br><span class="line">Subject: =?gb2312?B?ofEgzsrSu7K/zrrX2s3ytcS159Oww/uzxg==?=</span><br><span class="line">Date: Sun, 14 Aug 2005 10:16:47 +0800</span><br><span class="line">MIME-Version: 1.0</span><br><span class="line">Content-Type: text/plain;</span><br><span class="line">	charset=&quot;gb2312&quot;</span><br><span class="line">Content-Transfer-Encoding: base64</span><br><span class="line">X-Priority: 3</span><br><span class="line">X-MSMail-Priority: Normal</span><br><span class="line">X-Mailer: Microsoft Outlook Express 6.00.2800.1506</span><br><span class="line">X-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1506</span><br><span class="line"></span><br><span class="line">讲的是孔子后人的故事。一个老领导回到家乡，跟儿子感情不和，跟贪财的孙子孔为本和睦。</span><br><span class="line">老领导的弟弟魏宗万是赶马车的。</span><br><span class="line">有个洋妞大概是考察民俗的，在他们家过年。</span><br><span class="line">孔为本总想出国，被爷爷教育了。</span><br><span class="line">最后，一家人基本和解。</span><br><span class="line">顺便问另一类电影，北京青年电影制片厂的。中越战背景。一军人被介绍了一个对象，去相亲。女方是军队医院的护士，犹豫不决，总是在回忆战场上负伤的男友，好像还没死。最后</span><br><span class="line">男方表示理解，归队了。</span><br></pre></td></tr></table></figure></div>
<p>信息量非常大，在网上可以看到一些博主在训练模型时，直接将邮件头全部砍掉，只保留邮件体部分，将这个完整的数据集简化为同上的mini数据集。这怎么看都是不对的！哪怕它们跑出来的结果再好。<font color='	#00C5CD'><strong>我们要做的过滤器应该是同时考虑邮件头和邮件体两部分的内容的逻辑上完整的过滤器。</strong></font>这一部分的参考了<a href="#2%E5%8A%A0%E6%9D%83%E8%B4%9D%E5%8F%B6%E6%96%AF%E9%82%AE%E4%BB%B6%E8%BF%87%E6%BB%A4%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6">这篇</a>，所以在训练之前，需要搞清楚一封正常的电子邮件是由哪几部分构成的。</p>
<h2 id="先搞清楚，电子邮件！"><a href="#先搞清楚，电子邮件！" class="headerlink" title="先搞清楚，电子邮件！"></a>先搞清楚，电子邮件！</h2><p>这部分的内容比较杂，我就放一些查到的资料把。</p>
<div class="code-container" data-rel="Md"><figure class="iseeu highlight md"><table><tr><td class="code"><pre><span class="line"><span class="section"># 一封能在互联网上传播的电子邮件应包含的内容</span></span><br><span class="line"></span><br><span class="line">根据 <span class="strong">**RFC 822**</span> 和 <span class="strong">**MIME**</span> 标准，一封标准的电子邮件应包括以下几个组成部分：</span><br><span class="line"></span><br><span class="line"><span class="section">## 1. 邮件头部 (Headers)</span></span><br><span class="line">邮件头部包含了关于邮件本身和邮件通信过程的元信息。常见的邮件头部字段包括：</span><br><span class="line"></span><br><span class="line"><span class="section">### 1.1 发件人 (From)</span></span><br><span class="line"><span class="bullet">-</span> 指示邮件的发送者。</span><br><span class="line"><span class="bullet">-</span> 格式：<span class="code">`From: sender@example.com`</span></span><br><span class="line"><span class="bullet">-</span> 可以包括发件人的姓名，例如：<span class="code">`From: &quot;John Doe&quot; &lt;john.doe@example.com&gt;`</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 1.2 收件人 (To)</span></span><br><span class="line"><span class="bullet">-</span> 指定邮件的主要接收者。</span><br><span class="line"><span class="bullet">-</span> 格式：<span class="code">`To: recipient@example.com`</span></span><br><span class="line"><span class="bullet">-</span> 可以包括多个收件人，用逗号分隔。</span><br><span class="line"></span><br><span class="line"><span class="section">### 1.3 抄送 (CC) 和 密送 (BCC)</span></span><br><span class="line"><span class="bullet">-</span> <span class="strong">**抄送**</span>（CC）表示同时发送给其他人，而<span class="strong">**密送**</span>（BCC）则是发送给其他人但不公开其他收件人的地址。</span><br><span class="line"><span class="bullet">-</span> 格式：<span class="code">`CC: cc1@example.com, cc2@example.com`</span></span><br><span class="line"><span class="bullet">-</span> 格式：<span class="code">`BCC: bcc@example.com`</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 1.4 主题 (Subject)</span></span><br><span class="line"><span class="bullet">-</span> 邮件的主题行，简洁明了地概述邮件内容。</span><br><span class="line"><span class="bullet">-</span> 格式：<span class="code">`Subject: Meeting agenda for next week`</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 1.5 日期 (Date)</span></span><br><span class="line"><span class="bullet">-</span> 指明邮件的发送日期和时间。</span><br><span class="line"><span class="bullet">-</span> 格式：<span class="code">`Date: Wed, 21 Aug 2024 14:30:00 +0000`</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 1.6 邮件标识 (Message-ID)</span></span><br><span class="line"><span class="bullet">-</span> 每封邮件都会有一个唯一的标识符，帮助邮件系统跟踪邮件。</span><br><span class="line"><span class="bullet">-</span> 格式：<span class="code">`Message-ID: &lt;1234567890@example.com&gt;`</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 1.7 回复到 (Reply-To)</span></span><br><span class="line"><span class="bullet">-</span> 用于指定回复邮件时的目标地址，如果与发件人地址不同。</span><br><span class="line"><span class="bullet">-</span> 格式：<span class="code">`Reply-To: reply@example.com`</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 1.8 其他头部字段</span></span><br><span class="line"><span class="bullet">-</span> <span class="strong">**MIME-Version**</span>：标明邮件采用的 MIME 版本，通常是 <span class="code">`MIME-Version: 1.0`</span>。</span><br><span class="line"><span class="bullet">-</span> <span class="strong">**Content-Type**</span>：指定邮件内容的类型（如文本、HTML、附件等），在 MIME 邮件中非常重要。</span><br><span class="line"><span class="bullet">-</span> <span class="strong">**Content-Transfer-Encoding**</span>：描述邮件内容的编码方式（如 Base64 或 Quoted-Printable）。</span><br><span class="line"></span><br><span class="line"><span class="section">## 2. 邮件正文 (Body)</span></span><br><span class="line">邮件的正文是邮件的核心部分，包含具体的交流内容。正文的格式会根据 <span class="strong">**Content-Type**</span> 头部字段进行不同的处理。</span><br><span class="line"></span><br><span class="line"><span class="section">### 2.1 MIME 邮件的编码</span></span><br><span class="line"><span class="bullet">-</span> 如果邮件包含非 ASCII 字符或附件，它将使用 MIME（Multipurpose Internet Mail Extensions）标准进行编码，常见的编码方式有：</span><br><span class="line"><span class="bullet">  -</span> <span class="strong">**Base64**</span>：用于编码二进制数据（如图像、文档等附件）。</span><br><span class="line"><span class="bullet">  -</span> <span class="strong">**Quoted-Printable**</span>：用于编码较为简单的文本数据，避免控制字符导致邮件乱码。</span><br><span class="line">  </span><br><span class="line"><span class="section">### 2.2 文本邮件 (Text)</span></span><br><span class="line"><span class="bullet">-</span> <span class="strong">**纯文本**</span>（text/plain）邮件的正文内容通常是不带格式的文本。</span><br><span class="line"><span class="bullet">-</span> <span class="strong">**格式**</span>： <span class="code">`Content-Type: text/plain; charset=&quot;UTF-8&quot;`</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 2.3 HTML 邮件 (HTML)</span></span><br><span class="line"><span class="bullet">-</span> <span class="strong">**HTML 格式**</span>（text/html）邮件允许在正文中嵌入格式化文本、链接、图像等内容。</span><br><span class="line"><span class="bullet">-</span> <span class="strong">**格式**</span>： <span class="code">`Content-Type: text/html; charset=&quot;UTF-8&quot;`</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 2.4 多部分邮件 (Multipart)</span></span><br><span class="line"><span class="bullet">-</span> 如果邮件包含多个部分（例如，文本和附件），则会使用 <span class="strong">**multipart**</span> 类型的内容格式。</span><br><span class="line"><span class="bullet">-</span> <span class="strong">**格式**</span>：<span class="code">`Content-Type: multipart/mixed; boundary=&quot;boundary_string&quot;`</span></span><br><span class="line"></span><br><span class="line"><span class="section">## 3. 附件 (Attachments)</span></span><br><span class="line"><span class="bullet">-</span> 邮件可以包含附件文件，附件将通过 MIME 编码方式嵌入到邮件正文中。</span><br><span class="line"><span class="bullet">-</span> 每个附件都会有一个独立的内容描述，如 <span class="code">`Content-Type`</span> 和 <span class="code">`Content-Disposition`</span>。</span><br><span class="line"><span class="bullet">-</span> 示例：</span><br><span class="line">  <span class="code">```text</span></span><br><span class="line"><span class="code">  Content-Type: application/pdf; name=&quot;file.pdf&quot;</span></span><br><span class="line"><span class="code">  Content-Disposition: attachment; filename=&quot;file.pdf&quot;</span></span><br><span class="line"><span class="code">  Content-Transfer-Encoding: base64```</span></span><br><span class="line">  </span><br></pre></td></tr></table></figure></div>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%AF%95%E8%AE%BE%E6%9D%82%E8%AE%B0/email_sample.png"
                      alt="email_sample"
                ></p>
<p>由上述的种种可以看出，email的头中蕴含这丰富的信息可以用于判断邮件是否为spam，但是它和邮件体带来的信息又是不同的。</p>
]]></content>
      <categories>
        <category>毕设</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
