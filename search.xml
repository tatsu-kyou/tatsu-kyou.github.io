<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度学习——LeNet卷积神经网络初探</title>
    <url>/2024/11/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<h2 id="模型介绍："><a href="#模型介绍：" class="headerlink" title="模型介绍："></a>模型介绍：</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/p1.png"
                      alt="image"
                ></p>
<p><strong>简单介绍：</strong> 从网络结构可以看出LeNet对于现在的大模型来说是一个非常小的神经网络，他一共由7个层顺序连接组成。<font color="#845EC2">分别是卷积层、pooling层、卷积层、pooling层和三个全连接层</font>。用现代的深度学习框架来实现代码如下：  </p>
<h2 id="代码实现和解读："><a href="#代码实现和解读：" class="headerlink" title="代码实现和解读："></a>代码实现和解读：</h2><div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></div>

<p><strong>解读：</strong> 这一部分是有关网络的定义，可以看出网络的基本层实现都调用了torch的库，<code>sigmoid()</code> 函数的作用是：<font color="#845EC2">让网络中各层叠加后不会坍缩，因为引入了非线性函数。</font>我们来输出一下网络的各层的结构。</p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t:&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">Conv2d output shape: 	: torch.Size([1, 6, 28, 28])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 6, 28, 28])</span><br><span class="line">AvgPool2d output shape: : torch.Size([1, 6, 14, 14])</span><br><span class="line">Conv2d output shape: 	: torch.Size([1, 16, 10, 10])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 16, 10, 10])</span><br><span class="line">AvgPool2d output shape: : torch.Size([1, 16, 5, 5])</span><br><span class="line">Flatten output shape: 	: torch.Size([1, 400])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 120])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 120])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 84])</span><br><span class="line">Sigmoid output shape: 	: torch.Size([1, 84])</span><br><span class="line">Linear output shape: 	: torch.Size([1, 10])</span><br></pre></td></tr></table></figure></div>
<p>接下来我们利用沐神的<code>d2l</code>包中的数据集准备函数来下载<code>MNIST</code>数据集。</p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):  <span class="comment"># * 判断变量的类型</span></span><br><span class="line">        net.<span class="built_in">eval</span>()          </span><br><span class="line">        <span class="comment">#? 将网络设置为评估模式, 在此模式下，net会关闭一些特定的训练技巧以确保网络的行为和训练时一致</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)  <span class="comment"># * .to(device)是为了将数据送至指定的设备上进行计算</span></span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]  <span class="comment"># * 这里返回的是预测精度</span></span><br></pre></td></tr></table></figure></div>
<p>以上的这段代码的关键步骤是<font color="#FF6F91">执行了.to(device)操作，上述方法作用的调用可用的GPU进行加速运算</font>。  </p>
<p>接下来这段代码是对net执行训练的方法定义：  </p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)  <span class="comment"># ? 初始化参数</span></span><br><span class="line">            </span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()  <span class="comment"># ? 交叉熵损失函数</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">3</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">2</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()  <span class="comment"># ? 将网络设置为训练模式</span></span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):  <span class="comment"># ? enumerate会返回索引同时返回对应迭代次数时的元素</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span> <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>]*num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span> <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

<p>这段代码非常的长，我们将其分为几个部分来进行解读：  </p>
<p><strong>首先：</strong>  </p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)  <span class="comment"># ? 初始化参数</span></span><br><span class="line">            </span><br><span class="line">    net.apply(init_weights)</span><br></pre></td></tr></table></figure></div>
<p>这一段摘要做的是网络<font color='#FFC75F'>所有参数的初始化</font>。  </p>
<p><strong>其次：</strong></p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">net.to(device)</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.CrossEntropyLoss()  <span class="comment"># ? 交叉熵损失函数</span></span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">3</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">2</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br></pre></td></tr></table></figure></div>
<p>这一段主要是<font color='#FFC75F'>定义了网络训练和结果可视化的必要变量，并将网络放在GPU上进行运行</font>。</p>
<p><strong>接下来：</strong></p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">    net.train()  <span class="comment"># ? 将网络设置为训练模式</span></span><br><span class="line">    <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):  <span class="comment"># ? enumerate会返回索引同时返回对应迭代次数时的元素</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">        timer.stop()</span><br><span class="line">        train_l = metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line">        train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">            animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">    test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br></pre></td></tr></table></figure></div>
<p>这一部分是最重要的训练部分：<font color='#00C9A7'>前向传导、计算损失、对损失进行反向传导并计算梯度、根据梯度来更新参数</font>。对每一个样本都进行上述的基本过程。</p>
<p>剩下的部分就是对训练的中间过程进行适当的输出。</p>
<div class="highlight-container" data-rel="Py"><figure class="iseeu highlight py"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure></div>
<p>这段代码描述的是网络训练器使用的过程。根据上述参数定义，得到的训练结果如下图：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/p2.png"
                      alt="image"
                ></p>
<h2 id="模型局部最优化："><a href="#模型局部最优化：" class="headerlink" title="模型局部最优化："></a>模型局部最优化：</h2><p>接下来，我想做的是，利用循环和结果可视化来找到这个模型下的局部最优超参数。</p>
<p>–つづけ</p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>东游记(一)--此行伊始</title>
    <url>/2024/11/13/Journey-to-the-East1/</url>
    <content><![CDATA[<h2 id="写在最前"><a href="#写在最前" class="headerlink" title="写在最前"></a>写在最前</h2><p>折腾了好久，可算是把这个简陋的博客带起来了🤣。又回想起了早两年被编程环境支配的日子了。这次的博客是基于GitHub搭的，也许是感觉这样比博客园更高级？之前也没怎么接触过 <code>git</code>，就当是顺带学一下了。</p>
<p>特意开了一个新的 <code>categories</code> 用来记录 <strong>四非菜鸡如我的修士养成之路</strong> 的。哈哈，搞得好像我已经上岸开始写经验贴了一样，<font color='#FFC75F'>もう大丈夫だから、絶対に修士になるよ！</font></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/Journey-to-the-East1/p3.jpg"
                      alt="image"
                ></p>
<h2 id="为何出发"><a href="#为何出发" class="headerlink" title="为何出发"></a>为何出发</h2><p>该死的互联网留学机构，都什么年代了还在吹是个人都能润日。 </p>
<p>我想最初埋下种子是懵懂的大一，年轻时真是啥也不懂，不过从目前来看这个选择是相当正确的。时运感人，我本科班主任（下称“华子哥”）就是一个日留子。因为华子哥刚从海外回来，按照瑶湖女专的规定新任教师都要带一届本科生，同专业的其他班都配备专职辅导员，我这是什么运气😎。第一次班会华子哥就分享了自己的个人了经历，三年前的我一定做梦也想不到，三年后我将抛开所有国内可行的门路，开启东游之旅。</p>
<p>真正驱使我走上这条不归路的又是什么呢？我想，也许是那个明明很菜却又不想比任何人更烂的我在作祟，所以编织出了‘选择大于努力’的借口；也许是二〇年大家众所周知的原因，让国服考研、考公、考编成了三幻神，就业环境在两三年内断崖式崩坏；也许是当年的网络烂梗‘世界那么大 我想去看看’在搞鬼，没有出逃能力的前二十年积蓄了太多动力；也许是想再一次证明，除去高考，自己还有再去全力以赴做一件事的决心和能力。</p>
<p>我很功利，无法认可最开始就知道是毫无价值的事情，所以我无法接受国服的考研模式，润至少学会了一门语言？（政治、英语苦手你就知道我为啥接受不了了）。如果没有接触到润日，估计就是更保研狗拼刺刀或者是找工作去了。</p>
<p>另一方面，我没什么主见，保研、考研、找工作……我可能会在其中反复很跳，结果会怎么样真不太好说，所以我需要的是做减法，不给自己留下太多选项，这也是我在做选择时的一贯做法，<font color='#00C9A7'>永远搞不清自己想要什么，但是清楚的知道自己讨厌啥</font>。24年9月底保研狗上岸的时候，怎么说呢，心里不算平静吧，我的保研分应该是能够着末班车的，但我连系统都没填，也就没啥可焦虑的了嘿。但谁又能说我没有那么一瞬想着也许我本该可以休息呢？</p>
<p>我想我还是应了那句话 <code>心比天高 命比纸薄</code></p>
<h2 id="当下情况"><a href="#当下情况" class="headerlink" title="当下情况"></a>当下情况</h2><p>花了一年的时间通了两语，真是没一点语言天赋，踮起脚勉强够到出愿门槛。不过“速通”完接下来就是我比较擅长的了。相比日语，英语我是真的大苦手，至少现在微积分的日文教程能看懂个七七八八？绩点方面前面也说了我是可能有机会保研，不至于太烂，兴许得感谢我的前女友，至少前几年是读了点书的。ごめんなさい，年前的时候太不成熟，没分开说不定我现在已经躺了。往事不复提，恭喜你已经上岸，接下来就是我的主场了。</p>
<h2 id="当务之急"><a href="#当务之急" class="headerlink" title="当务之急"></a>当务之急</h2><p>等十一月的toeic结束后，就开始做毕设和套磁了，初定在年前搞定，套磁因为要套六所左右，进度没那么快，可以适当的延后。在这期间，同步复习修考的课程，数学四门左右and专业课四门左右，准备八到十门足以应对我想考的所有学校了。明年应该就是就是真正意义上的脱产备考了。<font color="#FF6F91">24年12月开始，属于我的修考之旅算是正式的开始了。</font></p>
<h2 id="暂此歇笔"><a href="#暂此歇笔" class="headerlink" title="暂此歇笔"></a>暂此歇笔</h2><p>我的古怪想法很多，但是真到输出的时候就变哑巴了。也许是因为我的灵魂还是过于干瘪，还得看更多的动漫玩更多的gal（bushi）。专栏不定期更新，少说多做，虽然我知道根本没人看，没什么期待才会偶遇惊喜嘛！じゃね、おやすみなさい～</p>
]]></content>
      <categories>
        <category>东游记</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>记录</tag>
      </tags>
  </entry>
  <entry>
    <title>git command</title>
    <url>/2024/12/01/git-command/</url>
    <content><![CDATA[<p><strong>一下全部内容均搬运于b站up“GeekHour”，仅个人学习使用</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/img/git-command/Git.png"
                      alt="git command"
                ></p>
]]></content>
      <categories>
        <category>常用命令</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title>本科毕设</title>
    <url>/2024/11/29/%E6%9C%AC%E7%A7%91%E6%AF%95%E8%AE%BE/</url>
    <content><![CDATA[<h2 id="毕设标题的灵感"><a href="#毕设标题的灵感" class="headerlink" title="毕设标题的灵感"></a>毕设标题的灵感</h2><p>其实没啥灵感可言，导通知说要定个题先应付一下学院的流程，遂上网冲浪寻找合适的题材。因为之后打算继续读研，javaweb相关的课题对我后续套磁的帮助不大，手头上又没有资源跑深度学习，所以打算做一些小体量的机器学习。正好那段时间听说outlook套磁容易进垃圾邮箱里，便对 <code>垃圾邮件的识别</code> 产生了一点兴趣(希望不是给自己挖的一个大坑)。所以就在不到一个小时的时间里草率的定下了毕设的课题。</p>
<h2 id="起步时遇到的问题"><a href="#起步时遇到的问题" class="headerlink" title="起步时遇到的问题"></a>起步时遇到的问题</h2><p>刚开始我以为这只是一个比较简单的小课题，也就没放在心上，等两语的考试结束以后正式开始的时候，太久没有接触专业知识的陌生感，我才发现我连电子邮箱是什么，工作的细节都一无所知。只能硬着头皮干了。  </p>
<ol>
<li>什么是电子邮件？电子邮件的生命周期？</li>
<li>什么是垃圾邮件？垃圾邮件的生命周期？</li>
<li>现在主流的垃圾邮件过滤器已经做到了什么程度？</li>
<li>数据集选什么样的比较合适？</li>
</ol>
<p>个人认为解决了上述的几个问题，会对这个课题有一个较为全面的了解。这个课题并不是非常新颖，在中文期刊中都已经有相当多的研究成果出现，所以有很多的问题可以在阅读文献后找到答案。</p>
<h2 id="文献阅读"><a href="#文献阅读" class="headerlink" title="文献阅读"></a>文献阅读</h2><h3 id="1-《个性化垃圾邮件过滤的扩展研究》"><a href="#1-《个性化垃圾邮件过滤的扩展研究》" class="headerlink" title="1.《个性化垃圾邮件过滤的扩展研究》"></a>1.《个性化垃圾邮件过滤的扩展研究》</h3><p>作者：徐丹丹，南京航空航天大学</p>
<p>是一篇硕士论文，在文章的绪论部分对上述的若干问题有较为详尽的解释，其中对于<code>垃圾邮件的生命周期</code>, <code>电子邮件的生命周期</code>的概念解释的较为清晰，结合GPT内容整理如下：</p>
<div class="highlight-container" data-rel="M"><figure class="iseeu highlight m"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>. 垃圾邮件的生命周期</span><br><span class="line">垃圾邮件是指未经请求发送给大量用户的广告或恶意邮件，通常用于推广产品、服务或执行恶意活动（如传播病毒、钓鱼攻击等）。垃圾邮件的生命周期包括几个阶段：</span><br><span class="line">    <span class="number">1.1</span> 域名注册阶段</span><br><span class="line">    攻击者选择域名：垃圾邮件发送者首先会选择并购买一个域名。这个域名通常看起来合法，但它们可能是用来建立一个虚假的网站，或者用于伪装成合法的公司或服务。通过注册域名，攻击者为垃圾邮件提供了一个发送的“地址”。</span><br><span class="line">    使用虚假信息：为了掩盖身份，垃圾邮件发送者可能会使用假信息注册域名，从而使追踪和防范变得更加困难。</span><br><span class="line">    <span class="number">1.2</span> DNS 名称解析和托管</span><br><span class="line">    DNS 配置：一旦攻击者注册了域名，他们需要配置DNS（域名系统）以便正确解析域名到特定的IP地址。DNS 服务器将负责将域名转换为计算机可识别的IP地址，指向一个Web 服务器。</span><br><span class="line">    网站托管：垃圾邮件发送者会搭建一个网站，该网站可能是一个恶意网站或虚假广告网站。这个网站可能包含恶意软件，或通过诱导用户点击链接进行钓鱼攻击。</span><br><span class="line">    <span class="number">1.3</span> 垃圾邮件的发送</span><br><span class="line">    僵尸网络：垃圾邮件的发送通常依赖于僵尸网络（botnets）。僵尸网络由大量受感染的计算机组成，攻击者通过它们来分发垃圾邮件。这些计算机通常是被恶意软件感染的，它们的所有者并不知道自己计算机的“角色”。</span><br><span class="line">    发送垃圾邮件：攻击者通过这些受控的计算机发送大量垃圾邮件，目的是让受害者点击邮件中的链接，访问恶意网站或下载恶意文件。</span><br><span class="line">    <span class="number">1.4</span> 诱使用户点击链接</span><br><span class="line">    钓鱼攻击：垃圾邮件中常常会嵌入恶意链接，这些链接指向伪装成合法网站的页面，目的是窃取用户个人信息、信用卡信息等。</span><br><span class="line">    诱导点击：通过各种手段（如假冒银行邮件、虚假优惠信息等），攻击者试图诱导用户点击邮件中的链接。</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>. 电子邮件的生命周期</span><br><span class="line">电子邮件的生命周期描述了从邮件创建到最终传递或丢弃的全过程。它通常涉及以下几个主要阶段：</span><br><span class="line">    <span class="number">2.1</span> 邮件创建</span><br><span class="line">    编写邮件：用户在电子邮件客户端（如Outlook、Gmail等）中编写一封邮件。邮件包括收件人、发件人、主题、正文内容以及附件。</span><br><span class="line">    发送邮件：用户点击发送按钮后，邮件通过邮件客户端发送到邮件服务器。</span><br><span class="line">    <span class="number">2.2</span> 邮件传输</span><br><span class="line">    发件人邮件服务器：邮件首先到达发件人的邮件服务器（通常是SMTP服务器），这是电子邮件传输的起点。</span><br><span class="line">    邮件路由：邮件服务器通过DNS查找目标邮件服务器的IP地址，并根据邮件的目的地进行路由。邮件可能经过多个中转服务器，特别是在不同域名之间发送邮件时。</span><br><span class="line">    中继服务器：如果目标邮件服务器不可用，邮件会通过中继服务器转发，直到到达最终目标服务器。</span><br><span class="line">    <span class="number">2.3</span> 邮件接收</span><br><span class="line">    接收方邮件服务器：邮件最终到达接收方的邮件服务器（通常是POP3或IMAP服务器）。接收方邮件服务器负责存储和管理邮件。</span><br><span class="line">    反垃圾邮件过滤：接收方邮件服务器会进行垃圾邮件过滤，检查邮件是否符合垃圾邮件的特征。如果是垃圾邮件，邮件会被拦截并放入垃圾邮件文件夹。</span><br><span class="line">    <span class="number">2.4</span> 邮件阅读</span><br><span class="line">    接收用户读取邮件：邮件最终到达接收方的电子邮件客户端，用户可以查看邮件内容。此时，邮件可能已经被标记为“已读”。</span><br><span class="line">    邮件响应：如果邮件是正常邮件，接收方可能会回复、转发邮件，或执行其他操作。如果邮件是垃圾邮件，接收方可能会选择标记为垃圾邮件或删除。</span><br><span class="line">    <span class="number">2.5</span> 邮件存档或删除</span><br><span class="line">    存档邮件：用户可以将重要邮件存档，以便以后查阅。</span><br><span class="line">    删除邮件：不再需要的邮件会被删除，垃圾邮件通常在这一阶段被清除。</span><br><span class="line">    <span class="number">2.6</span> 邮件丢失或失败传递</span><br><span class="line">    邮件失败传递：如果由于网络故障或邮件服务器问题，邮件无法送达目标地址，邮件系统会发送“传递失败”通知。</span><br><span class="line">    丢失邮件：在极少数情况下，由于邮件系统故障或配置错误，邮件可能在传输过程中丢失。</span><br></pre></td></tr></table></figure></div>

<p>此外，该文章还提出了<code>用户个性化过滤方案</code>，在传统的通用过滤器的筛选结果的基础上，基于多任务学习原理设计了两个相互学习的过滤器分别用于收件箱和垃圾箱的邮件的再过滤。这样做可以修正第一次过滤的结果，解决类不平均问题和用户对垃圾邮件的定义差异问题。</p>
<p>还有一个我比较在一的点，<code>广义虚漂移</code>，这个概念在目前看到的几篇文献中并未被提出，</p>
]]></content>
      <categories>
        <category>毕设</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Experience in improving spoken Japanese</title>
    <url>/2024/12/02/Experience-in-improving-spoken-Japanese/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>斥巨资5r买来的经验贴，感觉很实用所以整理好了放在自己的blog上，期待有缘人发现，当然如果有兴趣提升日语口语的欢迎和我talk😉😉😉🤩🤩🤩！</p>
<p>感觉提升一门语言听说的最快方法就是创造语境，这份经验贴也是如此，跟着下面的步骤走在国内也可以有语境，但大大大前提是要<code>坚持</code>下来。</p>
<h2 id="第一阶段：掌握单词、短句发音"><a href="#第一阶段：掌握单词、短句发音" class="headerlink" title="第一阶段：掌握单词、短句发音"></a>第一阶段：掌握单词、短句发音</h2><p>推荐时间:2个月<br>b站明王道跟动漫学标日单词<br>【【单词速记】跟动漫学新标日单词(初级上册合集)-哔哩哔哩】<a class="link"   href="https://b23.tv/tEkVa30" >https://b23.tv/tEkVa30 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>  </p>
<p>复述每句动漫台词，每天花一个半小时左右，直到把每个发音都听清</p>
<p>因为我在得到这份经验贴的时候刚过完N2，单词的发音这一块还有点印象的，所以这一块不会花太长的时间。但还是要过一遍的，毕竟基础不牢地动山摇。</p>
<h2 id="第二阶段：熟练不同场景下的对话"><a href="#第二阶段：熟练不同场景下的对话" class="headerlink" title="第二阶段：熟练不同场景下的对话"></a>第二阶段：熟练不同场景下的对话</h2><p>推荐时间：半年<br>最最剧场APP  </p>
<p>要求：必须选择1分钟左右动漫片段，台词十几句的进行配音(不要选择其他素材，后期用的时候动漫特定场景应用会浮现在脑海里，别的素材做不到) </p>
<p>数量：保质保量完成50个作品(每个作品练习时长至少半小时)  </p>
<p>听力巩固：每天听自己配过的作品，最好一个作品看几百遍</p>
<h2 id="第三阶段：日常对话训练"><a href="#第三阶段：日常对话训练" class="headerlink" title="第三阶段：日常对话训练"></a>第三阶段：日常对话训练</h2><p>推荐时间：不在日本的所有时间<br>Hellotalk APP  </p>
<p>找陌生日本人搭话，先文字聊天(学会使用日语输入法，聊几天就有组织句子的感觉了)，然后找人约打电话，最好对方会一些中文，找固定语伴，每周一小时，对话可达日常水平</p>
<h2 id="总结：无他，但手熟尔"><a href="#总结：无他，但手熟尔" class="headerlink" title="总结：无他，但手熟尔"></a>总结：无他，但手熟尔</h2><p>不必留于形式，看完这篇经验贴就把他忘了吧，你需要做的是花更多的时间去记单词、去组织语言、去交流。</p>
<p>日语学习中常会听到一句话<code>“N1过了就算入门了”</code>，是也是这个理，有没有这个N1都无所谓，大胆开口去说就完了！</p>
]]></content>
      <tags>
        <tag>日语</tag>
        <tag>经验贴</tag>
      </tags>
  </entry>
</search>
